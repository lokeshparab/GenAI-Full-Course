{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: textblob in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (0.18.0.post0)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "     -------------------------------------- 586.9/586.9 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.4 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Requirement already satisfied: joblib in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: click in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: tqdm in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from nltk>=3.8->textblob) (4.67.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas textblob emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45651</th>\n",
       "      <td>Even with the low standards of a dedicated hor...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31671</th>\n",
       "      <td>I enjoy the National Anthem. I enjoy the Natio...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27774</th>\n",
       "      <td>This film is the best kung fu film of all time...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22742</th>\n",
       "      <td>Mon Oncle Antoine observes the craggy face of ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18701</th>\n",
       "      <td>The Western society has been fed ideas about I...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "45651  Even with the low standards of a dedicated hor...  negative\n",
       "31671  I enjoy the National Anthem. I enjoy the Natio...  positive\n",
       "27774  This film is the best kung fu film of all time...  positive\n",
       "22742  Mon Oncle Antoine observes the craggy face of ...  positive\n",
       "18701  The Western society has been fed ideas about I...  negative"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "print(df['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercasing and Uppercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "exclude_puncatuation = string.punctuation\n",
    "exclude_puncatuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sstring   with punctuation\n",
      "sstring   with punctuation\n",
      "my name is lkehhhh\n",
      "my name is lkehhhh\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation_1(text:str):\n",
    "    return text.translate(str.maketrans(\"\", \"\", exclude_puncatuation))\n",
    "\n",
    "def remove_punctuation_2(text:str):\n",
    "    for char in exclude_puncatuation:\n",
    "        text = text.replace(char, \"\")\n",
    "    return text\n",
    "\n",
    "txt_1 = \"sstring @ *() with punctuation.!\"\n",
    "txt_2 = \"my name is l@ke$$$$$hhhh\"\n",
    "\n",
    "print(remove_punctuation_1(txt_1))\n",
    "print(remove_punctuation_2(txt_1))\n",
    "\n",
    "print(remove_punctuation_1(txt_2))\n",
    "print(remove_punctuation_2(txt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortform words (Gen Z words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for your information this is not true\n",
      "laugh my ass off the class was so funny\n",
      "I want it as soon as possible\n"
     ]
    }
   ],
   "source": [
    "txt_1 = \"FYI this is not true\"\n",
    "txt_2 = \"LMAO the class was so funny\"\n",
    "txt_3 = \"I want it ASAP\"\n",
    "\n",
    "chat_word = {\n",
    "    \"FYI\":\"for your information\",\n",
    "    \"LMAO\":\"laugh my ass off\",\n",
    "    \"ASAP\":\"as soon as possible\",\n",
    "    \"LOL\":\"laugh out loud\",\n",
    "    \"ROFL\":\"rolling on the floor laughing\",\n",
    "    \"ROFLMAO\":\"rolling on the floor laughing my ass off\",\n",
    "    \"LOLMAO\":\"laughing out loud my ass off\",\n",
    "    \"LOLWTF\":\"laughing out loud with a thumbs up\",\n",
    "    \"WTF\":\"with a thumbs up\",\n",
    "    \"ROFLWTF\":\"rolling on the floor laughing with a thumbs up\",\n",
    "    \"AFK\":\"away from keyboard\"\n",
    "}\n",
    "\n",
    "def chat_to_english(text:str):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_word:\n",
    "            new_text.append(chat_word[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    \n",
    "    return \" \".join(new_text)\n",
    "\n",
    "print(chat_to_english(txt_1))\n",
    "print(chat_to_english(txt_2))\n",
    "print(chat_to_english(txt_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "I Lokesh Parab learning NLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "list_of_stopwords=stopwords.words('english')\n",
    "print(list_of_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lokesh Parab learning NLP\n",
      "Hi Lokesh Parab working Data Scientist genai engineer. tell Loky?\n"
     ]
    }
   ],
   "source": [
    "txt_1 =  \"I am Lokesh Parab who is learning NLP\"\n",
    "txt_2 = \"Hi I am Lokesh Parab and i am working as Data Scientist and genai engineer. now tell me who is Loky?\"\n",
    "def remove_stopwords(text:str):\n",
    "    return \" \".join(\n",
    "        [\n",
    "            word.strip()\n",
    "            for word in text.split() \n",
    "            if word.lower() not in list_of_stopwords\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(remove_stopwords(txt_1))\n",
    "print(remove_stopwords(txt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my processing notebook please download this notebook'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install textblob as --> pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Error in text below\n",
    "text_1 = \"this is my processing notebook pleae download this ntebook\"\n",
    "\n",
    "blob = TextBlob(text_1)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here is my me that is sunny cavity and he is good motor'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not work properly for complex text error\n",
    "text_2 = \"here is my nme that is sunny savita and he is good mntor\"\n",
    "\n",
    "blob = TextBlob(text_2)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm brave ad strong person\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not work properly for complex text error\n",
    "text_3 = \"I'm brav ad stong prson\"\n",
    "\n",
    "blob = TextBlob(text_3)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,:smiling_face_with_smiling_eyes: how are you today? :glowing_star:\n",
      "Hello, :grinning_face_with_big_eyes::person_tipping_hand::grinning_face_with_big_eyes::person_tipping_hand: People\n",
      "•:bear::sunflower: Animals\n",
      "•:hamburger::tropical_drink: Food\n",
      "•:saxophone::soccer_ball: Activities\n",
      "•:oncoming_automobile::sunset: Travel\n",
      "•:light_bulb::party_popper: Objects\n",
      "•:sparkling_heart::input_symbols: Symbols\n",
      "•:crossed_flags::rainbow_flag: Flags\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "text_1 = \"Hello,😊 how are you today? 🌟\"\n",
    "text_2 = \"\"\"Hello, 😃💁😃💁 People\n",
    "•🐻🌻 Animals\n",
    "•🍔🍹 Food\n",
    "•🎷⚽ Activities\n",
    "•🚘🌇 Travel\n",
    "•💡🎉 Objects\n",
    "•💖🔣 Symbols\n",
    "•🎌🏳️‍🌈 Flags\"\"\"\n",
    "\n",
    "print(emoji.demojize(text_1))\n",
    "print(emoji.demojize(text_2))\n",
    "# print(emoji.demojize(text_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(emoji.is_emoji(\"😀\"))\n",
    "print(emoji.is_emoji(\"smiling face\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'lokesh', 'parab', 'are', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['i am lokesh parab are working as a data scientist']\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"i am lokesh parab are working as a data scientist\"\n",
    "\n",
    "#word tokenization\n",
    "print(text_1.split())\n",
    "\n",
    "#sentence tokenization\n",
    "print(text_1.split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'i',\n",
       " 'am',\n",
       " 'lokesh',\n",
       " 'parab',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'working',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'scientist']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi i am lokesh parab and i am working as a data scientist\"\n",
    "\n",
    "import re\n",
    "re.findall(r'\\w+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'i', 'am', 'lokesh', 'parab', 'and', 'i', 'am', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['hi i am lokesh parab and i am working as a data scientist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"hi i am lokesh parab and i am working as a data scientist\"\n",
    "\n",
    "print(word_tokenize(text))\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text_1 = \"the quick brown foxes jumping over the lazi dogs\"\n",
    "\n",
    "port_stemmer = PorterStemmer()\n",
    "\n",
    "[port_stemmer.stem(word) for word in text_1.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumping', 'over', 'the', 'lazi', 'dog']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text_1 = \"the quick brown foxes jumping over the lazi dogs\"\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "[lemmatizer.lemmatize(word) for word in text_1.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
