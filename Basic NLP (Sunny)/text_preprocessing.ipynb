{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: textblob in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (0.18.0.post0)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "     -------------------------------------- 586.9/586.9 kB 1.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.4 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Requirement already satisfied: joblib in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: click in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: tqdm in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from nltk>=3.8->textblob) (4.67.0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in e:\\githubs\\genai-full-course\\env\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas textblob emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18645</th>\n",
       "      <td>This film is an hour or so of good entertainme...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14367</th>\n",
       "      <td>Some films just fade away, but Tourist Trap ha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35187</th>\n",
       "      <td>great mystery, but the film goes down hill fro...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44213</th>\n",
       "      <td>There have been many (well, more than a few in...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40206</th>\n",
       "      <td>This movie features a pretty decent FX sequenc...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "18645  This film is an hour or so of good entertainme...  positive\n",
       "14367  Some films just fade away, but Tourist Trap ha...  positive\n",
       "35187  great mystery, but the film goes down hill fro...  negative\n",
       "44213  There have been many (well, more than a few in...  positive\n",
       "40206  This movie features a pretty decent FX sequenc...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "print(df['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercasing and Uppercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A WONDERFUL LITTLE PRODUCTION. <BR /><BR />THE FILMING TECHNIQUE IS VERY UNASSUMING- VERY OLD-TIME-BBC FASHION AND GIVES A COMFORTING, AND SOMETIMES DISCOMFORTING, SENSE OF REALISM TO THE ENTIRE PIECE. <BR /><BR />THE ACTORS ARE EXTREMELY WELL CHOSEN- MICHAEL SHEEN NOT ONLY \"HAS GOT ALL THE POLARI\" BUT HE HAS ALL THE VOICES DOWN PAT TOO! YOU CAN TRULY SEE THE SEAMLESS EDITING GUIDED BY THE REFERENCES TO WILLIAMS' DIARY ENTRIES, NOT ONLY IS IT WELL WORTH THE WATCHING BUT IT IS A TERRIFICLY WRITTEN AND PERFORMED PIECE. A MASTERFUL PRODUCTION ABOUT ONE OF THE GREAT MASTER'S OF COMEDY AND HIS LIFE. <BR /><BR />THE REALISM REALLY COMES HOME WITH THE LITTLE THINGS: THE FANTASY OF THE GUARD WHICH, RATHER THAN USE THE TRADITIONAL 'DREAM' TECHNIQUES REMAINS SOLID THEN DISAPPEARS. IT PLAYS ON OUR KNOWLEDGE AND OUR SENSES, PARTICULARLY WITH THE SCENES CONCERNING ORTON AND HALLIWELL AND THE SETS (PARTICULARLY OF THEIR FLAT WITH HALLIWELL'S MURALS DECORATING EVERY SURFACE) ARE TERRIBLY WELL DONE.\n"
     ]
    }
   ],
   "source": [
    "print(df['review'][1].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackle htmltags and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><title>Welcome to My Website</title><style>body{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}</style></head><body><header><h1>Welcome to My Awesome Website!</h1></header><main><p>This is a sample HTML document created for demonstration purposes.</p><p>Feel free to explore and enjoy the content on this website.</p></main><footer><p>&copy; 2024 My Website. All rights reserved.</p></footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><title>Welcome to My Website</title><style>body{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}</style></head><body><header><h1>Welcome to My Awesome Website!</h1></header><main><p>This is a sample HTML document created for demonstration purposes.</p><p>Feel free to explore and enjoy the content on this website.</p></main><footer><p>&copy; 2024 My Website. All rights reserved.</p></footer></body></html>\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern <.*?> is a regular expression (regex) used to match HTML tags. Here's a breakdown:\n",
    "\n",
    "<: Matches the opening angle bracket of an HTML tag.\n",
    "\n",
    ".*?: Matches any sequence of characters (. matches any character, * means zero or more occurrences), but the ? makes it non-greedy. This ensures it matches the shortest possible sequence between < and >.\n",
    "\n",
    ": Matches the closing angle bracket of an HTML tag.\n",
    "\n",
    "Example: For the input\n",
    "\n",
    "Hello\n",
    "\n",
    ", the pattern <.*?> will match:\n",
    "\n",
    "The function remove_html_tags replaces these matches with an empty string, effectively removing HTML tags from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to My Websitebody{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}Welcome to My Awesome Website!This is a sample HTML document created for demonstration purposes.Feel free to explore and enjoy the content on this website.&copy; 2024 My Website. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_html(text:str):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "print(remove_html(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production. The filming tec...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically there's a family where a little boy ...\n",
       "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    I'm going to have to disagree with the previou...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_html)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URL using regex\n",
    "\n",
    "The pattern r'https?://\\S+|www.\\S+' is a regular expression (regex) used to match URLs. Here's the explanation:\n",
    "\n",
    "https?://:\n",
    "\n",
    "Matches the literal \"http://\" or \"https://\". The s? makes the \"s\" optional, so it matches both \"http\" and \"https\". \\S+:\n",
    "\n",
    "Matches one or more non-whitespace characters (\\S means any character that is not a space). Ensures the URL continues until a space or end of the text. |:\n",
    "\n",
    "Acts as an OR operator. Matches either the first pattern (https?://\\S+) or the second. www.\\S+:\n",
    "\n",
    "Matches URLs starting with \"www.\", followed by one or more non-whitespace characters. The . matches the literal dot . after \"www\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkout my youtube channel \n",
      "checkout my youtube channel \n"
     ]
    }
   ],
   "source": [
    "text2=\"checkout my youtube channel https://www.youtube.com/watch?v=V9tJCQoBakA&t=225s\"\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_url_1(text:str):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_url_2(text:str):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "print(remove_url_1(text2))\n",
    "print(remove_url_2(text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "exclude_puncatuation = string.punctuation\n",
    "exclude_puncatuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sstring   with punctuation\n",
      "sstring   with punctuation\n",
      "my name is lkehhhh\n",
      "my name is lkehhhh\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation_1(text:str):\n",
    "    return text.translate(str.maketrans(\"\", \"\", exclude_puncatuation))\n",
    "\n",
    "def remove_punctuation_2(text:str):\n",
    "    for char in exclude_puncatuation:\n",
    "        text = text.replace(char, \"\")\n",
    "    return text\n",
    "\n",
    "txt_1 = \"sstring @ *() with punctuation.!\"\n",
    "txt_2 = \"my name is l@ke$$$$$hhhh\"\n",
    "\n",
    "print(remove_punctuation_1(txt_1))\n",
    "print(remove_punctuation_2(txt_1))\n",
    "\n",
    "print(remove_punctuation_1(txt_2))\n",
    "print(remove_punctuation_2(txt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortform words (Gen Z words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for your information this is not true\n",
      "laugh my ass off the class was so funny\n",
      "I want it as soon as possible\n"
     ]
    }
   ],
   "source": [
    "txt_1 = \"FYI this is not true\"\n",
    "txt_2 = \"LMAO the class was so funny\"\n",
    "txt_3 = \"I want it ASAP\"\n",
    "\n",
    "chat_word = {\n",
    "    \"FYI\":\"for your information\",\n",
    "    \"LMAO\":\"laugh my ass off\",\n",
    "    \"ASAP\":\"as soon as possible\",\n",
    "    \"LOL\":\"laugh out loud\",\n",
    "    \"ROFL\":\"rolling on the floor laughing\",\n",
    "    \"ROFLMAO\":\"rolling on the floor laughing my ass off\",\n",
    "    \"LOLMAO\":\"laughing out loud my ass off\",\n",
    "    \"LOLWTF\":\"laughing out loud with a thumbs up\",\n",
    "    \"WTF\":\"with a thumbs up\",\n",
    "    \"ROFLWTF\":\"rolling on the floor laughing with a thumbs up\",\n",
    "    \"AFK\":\"away from keyboard\"\n",
    "}\n",
    "\n",
    "def chat_to_english(text:str):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_word:\n",
    "            new_text.append(chat_word[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    \n",
    "    return \" \".join(new_text)\n",
    "\n",
    "print(chat_to_english(txt_1))\n",
    "print(chat_to_english(txt_2))\n",
    "print(chat_to_english(txt_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "NLTK\n",
    "\n",
    "TextBlob\n",
    "\n",
    "Spacy\n",
    "\n",
    "this library have Huge funcationality\n",
    "\n",
    "But we just gonna understand the basics and how to use it\n",
    "install text blob with pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK ( Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "I Lokesh Parab learning NLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "list_of_stopwords=stopwords.words('english')\n",
    "print(list_of_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lokesh Parab learning NLP\n",
      "Hi Lokesh Parab working Data Scientist genai engineer. tell Loky?\n"
     ]
    }
   ],
   "source": [
    "txt_1 =  \"I am Lokesh Parab who is learning NLP\"\n",
    "txt_2 = \"Hi I am Lokesh Parab and i am working as Data Scientist and genai engineer. now tell me who is Loky?\"\n",
    "def remove_stopwords(text:str):\n",
    "    return \" \".join(\n",
    "        [\n",
    "            word.strip()\n",
    "            for word in text.split() \n",
    "            if word.lower() not in list_of_stopwords\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(remove_stopwords(txt_1))\n",
    "print(remove_stopwords(txt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my processing notebook please download this notebook'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install textblob as --> pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Error in text below\n",
    "text_1 = \"this is my processing notebook pleae download this ntebook\"\n",
    "\n",
    "blob = TextBlob(text_1)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here is my me that is sunny cavity and he is good motor'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not work properly for complex text error\n",
    "text_2 = \"here is my nme that is sunny savita and he is good mntor\"\n",
    "\n",
    "blob = TextBlob(text_2)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm brave ad strong person\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not work properly for complex text error\n",
    "text_3 = \"I'm brav ad stong prson\"\n",
    "\n",
    "blob = TextBlob(text_3)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,:smiling_face_with_smiling_eyes: how are you today? :glowing_star:\n",
      "Hello, :grinning_face_with_big_eyes::person_tipping_hand::grinning_face_with_big_eyes::person_tipping_hand: People\n",
      "•:bear::sunflower: Animals\n",
      "•:hamburger::tropical_drink: Food\n",
      "•:saxophone::soccer_ball: Activities\n",
      "•:oncoming_automobile::sunset: Travel\n",
      "•:light_bulb::party_popper: Objects\n",
      "•:sparkling_heart::input_symbols: Symbols\n",
      "•:crossed_flags::rainbow_flag: Flags\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "text_1 = \"Hello,😊 how are you today? 🌟\"\n",
    "text_2 = \"\"\"Hello, 😃💁😃💁 People\n",
    "•🐻🌻 Animals\n",
    "•🍔🍹 Food\n",
    "•🎷⚽ Activities\n",
    "•🚘🌇 Travel\n",
    "•💡🎉 Objects\n",
    "•💖🔣 Symbols\n",
    "•🎌🏳️‍🌈 Flags\"\"\"\n",
    "\n",
    "print(emoji.demojize(text_1))\n",
    "print(emoji.demojize(text_2))\n",
    "# print(emoji.demojize(text_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(emoji.is_emoji(\"😀\"))\n",
    "print(emoji.is_emoji(\"smiling face\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'lokesh', 'parab', 'are', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['i am lokesh parab are working as a data scientist']\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"i am lokesh parab are working as a data scientist\"\n",
    "\n",
    "#word tokenization\n",
    "print(text_1.split())\n",
    "\n",
    "#sentence tokenization\n",
    "print(text_1.split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'i',\n",
       " 'am',\n",
       " 'lokesh',\n",
       " 'parab',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'working',\n",
       " 'as',\n",
       " 'a',\n",
       " 'data',\n",
       " 'scientist']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi i am lokesh parab and i am working as a data scientist\"\n",
    "\n",
    "import re\n",
    "re.findall(r'\\w+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'i', 'am', 'lokesh', 'parab', 'and', 'i', 'am', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['hi i am lokesh parab and i am working as a data scientist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"hi i am lokesh parab and i am working as a data scientist\"\n",
    "\n",
    "print(word_tokenize(text))\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmetization\n",
    "keeping the data into the root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text_1 = \"the quick brown foxes jumping over the lazi dogs\"\n",
    "\n",
    "port_stemmer = PorterStemmer()\n",
    "\n",
    "[port_stemmer.stem(word) for word in text_1.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumping', 'over', 'the', 'lazi', 'dog']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text_1 = \"the quick brown foxes jumping over the lazi dogs\"\n",
    "\n",
    "def lammatization(text:str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in text.split()\n",
    "    ]\n",
    "\n",
    "lammatization(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Ikigai'\",\n",
       " 'by',\n",
       " 'Hector',\n",
       " 'Garcia',\n",
       " 'and',\n",
       " 'Francesc',\n",
       " 'Miralles',\n",
       " 'explores',\n",
       " 'the',\n",
       " 'Japanese',\n",
       " 'concept',\n",
       " 'of',\n",
       " 'finding',\n",
       " \"one's\",\n",
       " 'purpose',\n",
       " 'in',\n",
       " 'life',\n",
       " 'by',\n",
       " 'analyzing',\n",
       " 'the',\n",
       " 'habit',\n",
       " 'and',\n",
       " 'belief',\n",
       " 'of',\n",
       " 'the',\n",
       " \"world's\",\n",
       " 'longest-living',\n",
       " 'people.',\n",
       " 'Through',\n",
       " 'case',\n",
       " 'studies,',\n",
       " 'the',\n",
       " 'book',\n",
       " 'offer',\n",
       " 'practical',\n",
       " 'insight',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'live',\n",
       " 'a',\n",
       " 'more',\n",
       " 'fulfilling',\n",
       " 'life.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"'Ikigai' by Hector Garcia and Francesc Miralles explores the Japanese concept of finding one's purpose in life by analyzing the habits and beliefs of the world's longest-living people. Through case studies, the book offers practical insights on how to live a more fulfilling life.\"\n",
    "\n",
    "lammatization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
