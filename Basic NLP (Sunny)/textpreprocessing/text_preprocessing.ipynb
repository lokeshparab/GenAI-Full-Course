{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/Ankit152/IMDB-sentiment-analysis/master/IMDB-Dataset.csv\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38446</th>\n",
       "      <td>It's not quite the timeless masterpiece you wo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37193</th>\n",
       "      <td>If you have any clue about Jane Austen´s produ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16437</th>\n",
       "      <td>In New York, in a morning close to Christmas, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12286</th>\n",
       "      <td>This miniseries is a reasonable sequel to the ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25131</th>\n",
       "      <td>No movie I've ever seen before has even come c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "38446  It's not quite the timeless masterpiece you wo...  positive\n",
       "37193  If you have any clue about Jane Austen´s produ...  positive\n",
       "16437  In New York, in a morning close to Christmas, ...  negative\n",
       "12286  This miniseries is a reasonable sequel to the ...  positive\n",
       "25131  No movie I've ever seen before has even come c...  negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n"
     ]
    }
   ],
   "source": [
    "print(df['review'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercasing and Uppercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A WONDERFUL LITTLE PRODUCTION. <BR /><BR />THE FILMING TECHNIQUE IS VERY UNASSUMING- VERY OLD-TIME-BBC FASHION AND GIVES A COMFORTING, AND SOMETIMES DISCOMFORTING, SENSE OF REALISM TO THE ENTIRE PIECE. <BR /><BR />THE ACTORS ARE EXTREMELY WELL CHOSEN- MICHAEL SHEEN NOT ONLY \"HAS GOT ALL THE POLARI\" BUT HE HAS ALL THE VOICES DOWN PAT TOO! YOU CAN TRULY SEE THE SEAMLESS EDITING GUIDED BY THE REFERENCES TO WILLIAMS' DIARY ENTRIES, NOT ONLY IS IT WELL WORTH THE WATCHING BUT IT IS A TERRIFICLY WRITTEN AND PERFORMED PIECE. A MASTERFUL PRODUCTION ABOUT ONE OF THE GREAT MASTER'S OF COMEDY AND HIS LIFE. <BR /><BR />THE REALISM REALLY COMES HOME WITH THE LITTLE THINGS: THE FANTASY OF THE GUARD WHICH, RATHER THAN USE THE TRADITIONAL 'DREAM' TECHNIQUES REMAINS SOLID THEN DISAPPEARS. IT PLAYS ON OUR KNOWLEDGE AND OUR SENSES, PARTICULARLY WITH THE SCENES CONCERNING ORTON AND HALLIWELL AND THE SETS (PARTICULARLY OF THEIR FLAT WITH HALLIWELL'S MURALS DECORATING EVERY SURFACE) ARE TERRIBLY WELL DONE.\n"
     ]
    }
   ],
   "source": [
    "print(df['review'][1].upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ONE OF THE OTHER REVIEWERS HAS MENTIONED THAT ...\n",
       "1        A WONDERFUL LITTLE PRODUCTION. <BR /><BR />THE...\n",
       "2        I THOUGHT THIS WAS A WONDERFUL WAY TO SPEND TI...\n",
       "3        BASICALLY THERE'S A FAMILY WHERE A LITTLE BOY ...\n",
       "4        PETTER MATTEI'S \"LOVE IN THE TIME OF MONEY\" IS...\n",
       "                               ...                        \n",
       "49995    I THOUGHT THIS MOVIE DID A DOWN RIGHT GOOD JOB...\n",
       "49996    BAD PLOT, BAD DIALOGUE, BAD ACTING, IDIOTIC DI...\n",
       "49997    I AM A CATHOLIC TAUGHT IN PAROCHIAL ELEMENTARY...\n",
       "49998    I'M GOING TO HAVE TO DISAGREE WITH THE PREVIOU...\n",
       "49999    NO ONE EXPECTS THE STAR TREK MOVIES TO BE HIGH...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackle htmltags and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><title>Welcome to My Website</title><style>body{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}</style></head><body><header><h1>Welcome to My Awesome Website!</h1></header><main><p>This is a sample HTML document created for demonstration purposes.</p><p>Feel free to explore and enjoy the content on this website.</p></main><footer><p>&copy; 2024 My Website. All rights reserved.</p></footer></body></html>\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"><title>Welcome to My Website</title><style>body{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}</style></head><body><header><h1>Welcome to My Awesome Website!</h1></header><main><p>This is a sample HTML document created for demonstration purposes.</p><p>Feel free to explore and enjoy the content on this website.</p></main><footer><p>&copy; 2024 My Website. All rights reserved.</p></footer></body></html>\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern <.*?> is a regular expression (regex) used to match HTML tags. Here's a breakdown:\n",
    "\n",
    "<: Matches the opening angle bracket of an HTML tag.\n",
    "\n",
    ".*?: Matches any sequence of characters (. matches any character, * means zero or more occurrences), but the ? makes it non-greedy. This ensures it matches the shortest possible sequence between < and >.\n",
    "\n",
    ": Matches the closing angle bracket of an HTML tag.\n",
    "\n",
    "Example: For the input\n",
    "\n",
    "Hello\n",
    "\n",
    ", the pattern <.*?> will match:\n",
    "\n",
    "The function remove_html_tags replaces these matches with an empty string, effectively removing HTML tags from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to My Websitebody{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}Welcome to My Awesome Website!This is a sample HTML document created for demonstration purposes.Feel free to explore and enjoy the content on this website.&copy; 2024 My Website. All rights reserved.\n",
      "\n",
      "Welcome to My Websitebody{font-family:'Arial',sans-serif;background-color:#f0f0f0;color:#333;margin:20px}h1{color:#007bff}p{line-height:1.5}Welcome to My Awesome Website!This is a sample HTML document created for demonstration purposes.Feel free to explore and enjoy the content on this website.&copy; 2024 My Website. All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags_1(text:str):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "\n",
    "def remove_html_tags_2(text:str):\n",
    "    pattern = re.compile(r'<.*?>')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "print(remove_html_tags_1(text), end=\"\\n\\n\")\n",
    "print(remove_html_tags_2(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production. The filming tec...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically there's a family where a little boy ...\n",
       "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    I'm going to have to disagree with the previou...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags_1)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove URL using regex\n",
    "\n",
    "The pattern `r'https?://\\S+|www.\\S+'` is a regular expression (regex) used to match URLs. Here's the explanation:\n",
    "\n",
    "`https?://`:\n",
    "\n",
    "Matches the literal `\"http://\"` or `\"https://\"`. The `s?` makes the `\"s\"` optional, so it matches both `\"http\"` and `\"https\". \\S+`:\n",
    "\n",
    "Matches one or more non-whitespace characters (`\\S` means any character that is not a space). Ensures the URL continues until a space or end of the text. `|`:\n",
    "\n",
    "Acts as an OR operator. Matches either the first pattern (`https?://\\S+`) or the second. `www.\\S+`:\n",
    "\n",
    "Matches URLs starting with `\"www.\"`, followed by one or more non-whitespace characters. The . matches the literal dot `.` after `\"www\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkout my youtube channel \n",
      "checkout my youtube channel \n"
     ]
    }
   ],
   "source": [
    "text2=\"checkout my youtube channel https://www.youtube.com/watch?v=V9tJCQoBakA&t=225s\"\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_url_1(text:str):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_url_2(text:str):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "print(remove_url_1(text2))\n",
    "print(remove_url_2(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production. The filming tec...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically there's a family where a little boy ...\n",
       "4        Petter Mattei's \"Love in the Time of Money\" is...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    I'm going to have to disagree with the previou...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_url_1)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "exclude_puncatuation = string.punctuation\n",
    "exclude_puncatuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sstring   with punctuation\n",
      "sstring   with punctuation\n",
      "my name is lkehhhh\n",
      "my name is lkehhhh\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation_1(text:str):\n",
    "    return text.translate(str.maketrans(\"\", \"\", exclude_puncatuation))\n",
    "\n",
    "def remove_punctuation_2(text:str):\n",
    "    for char in exclude_puncatuation:\n",
    "        text = text.replace(char, \"\")\n",
    "    return text\n",
    "\n",
    "txt_1 = \"sstring @ *() with punctuation.!\"\n",
    "txt_2 = \"my name is l@ke$$$$$hhhh\"\n",
    "\n",
    "print(remove_punctuation_1(txt_1))\n",
    "print(remove_punctuation_2(txt_1))\n",
    "\n",
    "print(remove_punctuation_1(txt_2))\n",
    "print(remove_punctuation_2(txt_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production The filming tech...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically theres a family where a little boy J...\n",
       "4        Petter Matteis Love in the Time of Money is a ...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot bad dialogue bad acting idiotic direc...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    Im going to have to disagree with the previous...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_punctuation_1)\n",
    "df['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shortform words (Gen Z words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for your information this is not true\n",
      "laugh my ass off the class was so funny\n",
      "I want it as soon as possible\n"
     ]
    }
   ],
   "source": [
    "txt_1 = \"FYI this is not true\"\n",
    "txt_2 = \"LMAO the class was so funny\"\n",
    "txt_3 = \"I want it ASAP\"\n",
    "\n",
    "chat_word = {\n",
    "    \"FYI\":\"for your information\",\n",
    "    \"LMAO\":\"laugh my ass off\",\n",
    "    \"ASAP\":\"as soon as possible\",\n",
    "    \"LOL\":\"laugh out loud\",\n",
    "    \"ROFL\":\"rolling on the floor laughing\",\n",
    "    \"ROFLMAO\":\"rolling on the floor laughing my ass off\",\n",
    "    \"LOLMAO\":\"laughing out loud my ass off\",\n",
    "    \"LOLWTF\":\"laughing out loud with a thumbs up\",\n",
    "    \"WTF\":\"with a thumbs up\",\n",
    "    \"ROFLWTF\":\"rolling on the floor laughing with a thumbs up\",\n",
    "    \"AFK\":\"away from keyboard\"\n",
    "}\n",
    "\n",
    "def chat_to_english(text:str):\n",
    "        \n",
    "    return \" \".join([\n",
    "        (\n",
    "            chat_word.get(word.upper(),word)\n",
    "            if word.upper() in chat_word \n",
    "            else word\n",
    "        )\n",
    "        for word in text.split()\n",
    "    ])\n",
    "\n",
    "print(chat_to_english(txt_1))\n",
    "print(chat_to_english(txt_2))\n",
    "print(chat_to_english(txt_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "1. NLTK\n",
    "\n",
    "2. TextBlob\n",
    "\n",
    "3. Spacy\n",
    "\n",
    "This library have Huge funcationality\n",
    "\n",
    "But we just gonna understand the basics and how to use it\n",
    "install text blob with pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK ( Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "list_of_stopwords=stopwords.words('english')\n",
    "print(list_of_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lokesh Parab learning NLP\n",
      "Hi Lokesh Parab working Data Scientist genai engineer. tell Loky?\n"
     ]
    }
   ],
   "source": [
    "txt_1 =  \"I am Lokesh Parab who is learning NLP\"\n",
    "txt_2 = \"Hi I am Lokesh Parab and i am working as Data Scientist and genai engineer. now tell me who is Loky?\"\n",
    "def remove_stopwords(text:str):\n",
    "    return \" \".join(\n",
    "        [\n",
    "            word.strip()\n",
    "            for word in text.split() \n",
    "            if word.lower() not in list_of_stopwords\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(remove_stopwords(txt_1))\n",
    "print(remove_stopwords(txt_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Blob\n",
    "\n",
    "**Text Blob** is simple intelligent model which is used to make spelling correction in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is my processing notebook please download this notebook'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install textblob as --> pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Error in text below\n",
    "text_1 = \"this is my procesing notebook pleae download this ntebook\"\n",
    "\n",
    "blob = TextBlob(text_1)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here is my me that is sunny cavity and he is good motor'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not work properly for complex text error\n",
    "text_2 = \"here is my nme that is sunny savita and he is good mntor\"\n",
    "\n",
    "blob = TextBlob(text_2)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm brave ad strong person\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does not work properly for complex text error\n",
    "text_3 = \"I'm brav ad stong prson\"\n",
    "\n",
    "blob = TextBlob(text_3)\n",
    "blob.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,:smiling_face_with_smiling_eyes: how are you today? :glowing_star:\n",
      "Hello, :grinning_face_with_big_eyes::person_tipping_hand::grinning_face_with_big_eyes::person_tipping_hand: People\n",
      "•:bear::sunflower: Animals\n",
      "•:hamburger::tropical_drink: Food\n",
      "•:saxophone::soccer_ball: Activities\n",
      "•:oncoming_automobile::sunset: Travel\n",
      "•:light_bulb::party_popper: Objects\n",
      "•:sparkling_heart::input_symbols: Symbols\n",
      "•:crossed_flags::rainbow_flag: Flags\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "text_1 = \"Hello,😊 how are you today? 🌟\"\n",
    "text_2 = \"\"\"Hello, 😃💁😃💁 People\n",
    "•🐻🌻 Animals\n",
    "•🍔🍹 Food\n",
    "•🎷⚽ Activities\n",
    "•🚘🌇 Travel\n",
    "•💡🎉 Objects\n",
    "•💖🔣 Symbols\n",
    "•🎌🏳️‍🌈 Flags\"\"\"\n",
    "\n",
    "print(emoji.demojize(text_1))\n",
    "print(emoji.demojize(text_2))\n",
    "# print(emoji.demojize(text_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(emoji.is_emoji(\"😀\"))\n",
    "print(emoji.is_emoji(\"smiling face\"))\n",
    "print(emoji.is_emoji(\":grinning_face_with_big_eyes:\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'lokesh', 'parab', 'are', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['i am lokesh parab are working as a data scientist']\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"i am lokesh parab are working as a data scientist\"\n",
    "\n",
    "#word tokenization\n",
    "print(remove_punctuation_1(text_1).split())\n",
    "\n",
    "#sentence tokenization\n",
    "print(text_1.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'lokesh', 'parab', 'are', 'working', 'as', 'a', 'data', 'scientist', 'i', 'live', 'in', 'mumbai', 'and', 'working', 'into', 'multiple', 'domains']\n",
      "['i am lokesh parab are working as a data scientist', ' i live in mumbai and working into multiple domains', '']\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"i am lokesh parab are working as a data scientist. i live in mumbai and working into multiple domains.\"\n",
    "\n",
    "#word tokenization\n",
    "print(remove_punctuation_1(text_2).split())\n",
    "\n",
    "#sentence tokenization\n",
    "print(text_2.split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'i', 'am', 'lokesh', 'parab', 'and', 'i', 'am', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['i', 'am', 'lokesh', 'parab', 'are', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['i', 'am', 'lokesh', 'parab', 'are', 'working', 'as', 'a', 'data', 'scientist', 'i', 'live', 'in', 'mumbai', 'and', 'working', 'into', 'multiple', 'domains']\n"
     ]
    }
   ],
   "source": [
    "text = \"hi i am lokesh parab and i am working as a data scientist\"\n",
    "\n",
    "import re\n",
    "print(re.findall(r'\\w+', text))\n",
    "print(re.findall(r'[\\w]+', text_1))\n",
    "print(re.findall(r'\\w+', text_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** :- Langchain has many *TextSplitter*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'i', 'am', 'lokesh', 'parab', 'and', 'i', 'am', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['hi i am lokesh parab and i am working as a data scientist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"hi i am lokesh parab and i am working as a data scientist\"\n",
    "\n",
    "print(word_tokenize(text))\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'lokesh', 'parab', 'are', 'working', 'as', 'a', 'data', 'scientist']\n",
      "['i am lokesh parab are working as a data scientist']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text_1))\n",
    "print(sent_tokenize(text_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'lokesh', 'parab', 'are', 'working', 'as', 'a', 'data', 'scientist', '.', 'i', 'live', 'in', 'mumbai', 'and', 'working', 'into', 'multiple', 'domains', '.']\n",
      "['i am lokesh parab are working as a data scientist.', 'i live in mumbai and working into multiple domains.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text_2))\n",
    "print(sent_tokenize(text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Generative artificial intelligence (generative AI, genAI, GenAI, GAI or GenAI[1]) is artificial intelligence capable of generating text, images or other data using generative models,[2] often in response to prompts.',\n",
       " '[3][4] Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.',\n",
       " '[5][6]\\n\\nImprovements in transformer-based deep neural networks enabled an AI boom of generative AI systems in the early 2020s.',\n",
       " 'These include large language model (LLM) chatbots such as ChatGPT, Copilot, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.[7][8][9] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.',\n",
       " '[3][10][11]']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpus=\"\"\"Generative artificial intelligence (generative AI, genAI, GenAI, GAI or GenAI[1]) is artificial intelligence capable of generating text, images or other data using generative models,[2] often in response to prompts.[3][4] Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics.[5][6]\n",
    "\n",
    "Improvements in transformer-based deep neural networks enabled an AI boom of generative AI systems in the early 2020s. These include large language model (LLM) chatbots such as ChatGPT, Copilot, Bard, and LLaMA, and text-to-image artificial intelligence art systems such as Stable Diffusion, Midjourney, and DALL-E.[7][8][9] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[3][10][11]\"\"\"\n",
    "\n",
    "sent_tokenize(my_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmetization\n",
    "keeping the data into the root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text_1 = \"the quick brown foxes jumping over the lazi dogs\"\n",
    "\n",
    "port_stemmer = PorterStemmer()\n",
    "\n",
    "[port_stemmer.stem(word) for word in text_1.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumping', 'over', 'the', 'lazi', 'dog']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text_1 = \"the quick brown foxes jumping over the lazi dogs\"\n",
    "\n",
    "def lammatization(text:str):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [\n",
    "        lemmatizer.lemmatize(word) \n",
    "        for word in text.split()\n",
    "    ]\n",
    "\n",
    "lammatization(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'Ikigai'\",\n",
       " 'by',\n",
       " 'Hector',\n",
       " 'Garcia',\n",
       " 'and',\n",
       " 'Francesc',\n",
       " 'Miralles',\n",
       " 'explores',\n",
       " 'the',\n",
       " 'Japanese',\n",
       " 'concept',\n",
       " 'of',\n",
       " 'finding',\n",
       " \"one's\",\n",
       " 'purpose',\n",
       " 'in',\n",
       " 'life',\n",
       " 'by',\n",
       " 'analyzing',\n",
       " 'the',\n",
       " 'habit',\n",
       " 'and',\n",
       " 'belief',\n",
       " 'of',\n",
       " 'the',\n",
       " \"world's\",\n",
       " 'longest-living',\n",
       " 'people.',\n",
       " 'Through',\n",
       " 'case',\n",
       " 'studies,',\n",
       " 'the',\n",
       " 'book',\n",
       " 'offer',\n",
       " 'practical',\n",
       " 'insight',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'live',\n",
       " 'a',\n",
       " 'more',\n",
       " 'fulfilling',\n",
       " 'life.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"'Ikigai' by Hector Garcia and Francesc Miralles explores the Japanese concept of finding one's purpose in life by analyzing the habits and beliefs of the world's longest-living people. Through case studies, the book offers practical insights on how to live a more fulfilling life.\"\n",
    "\n",
    "lammatization(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
