{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVpCNGcLLegfHmiL7OcfuT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokeshparab/GenAI-Full-Course/blob/main/RAG/MM_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Library"
      ],
      "metadata": {
        "id": "pq4iMa7Ki5gV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erQH16gAgNV8",
        "outputId": "8975817d-89d4-44b7-d3ea-4c08ddfbaf3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (18.154.101.94)] [Co\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Waiting for headers] \r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: unstructured-pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.15)\n",
            "Requirement already satisfied: unstructured[all-docs] in /usr/local/lib/python3.11/dist-packages (0.17.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (0.4.27)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (4.13.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (2.0.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (3.13.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (4.13.2)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (0.34.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.1)\n",
            "Requirement already satisfied: onnxruntime>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.22.0)\n",
            "Requirement already satisfied: python-pptx>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.0.2)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (0.4.1)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.15)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (9.7.0)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (3.10.1)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (20250506)\n",
            "Requirement already satisfied: unstructured-inference>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (0.8.10)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (3.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (2.2.2)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.17.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (3.1.5)\n",
            "Requirement already satisfied: python-docx>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.1.2)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (5.5.0)\n",
            "Requirement already satisfied: onnx>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (1.18.0)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (3.8)\n",
            "Requirement already satisfied: pi-heif in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]) (0.22.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.17.0->unstructured[all-docs]) (5.29.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[all-docs]) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx>=1.0.1->unstructured[all-docs]) (3.2.3)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[all-docs]) (0.0.20)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[all-docs]) (0.31.1)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[all-docs]) (4.11.0.86)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[all-docs]) (2.6.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[all-docs]) (1.0.15)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[all-docs]) (4.51.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[all-docs]) (1.15.3)\n",
            "Requirement already satisfied: pypdfium2 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=0.8.10->unstructured[all-docs]) (4.30.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[all-docs]) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[all-docs]) (0.21.0+cu124)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[all-docs]) (2.0.8)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[all-docs]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.26.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[all-docs]) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[all-docs]) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[all-docs]) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[all-docs]) (2024.11.6)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl->unstructured[all-docs]) (2.0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured[all-docs]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured[all-docs]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[all-docs]) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[all-docs]) (43.0.3)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from pikepdf->unstructured[all-docs]) (1.2.18)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured[all-docs]) (0.47)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[all-docs]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[all-docs]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[all-docs]) (2025.4.26)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[all-docs]) (24.1.0)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[all-docs]) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[all-docs]) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (0.16.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]) (6.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->unstructured-inference>=0.8.10->unstructured[all-docs]) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (3.18.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.19.0->unstructured[all-docs]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference>=0.8.10->unstructured[all-docs]) (0.21.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured-inference>=0.8.10->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.19.0->unstructured[all-docs]) (10.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[all-docs]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unstructured-inference>=0.8.10->unstructured[all-docs]) (3.0.2)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: grandalf in /usr/local/lib/python3.11/dist-packages (0.8)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: langchain-pinecone in /usr/local/lib/python3.11/dist-packages (0.2.6)\n",
            "Requirement already satisfied: pinecone-notebooks in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: langchain-anthropic in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.4)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.33.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from grandalf) (3.2.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.10.11)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.24.0)\n",
            "Requirement already satisfied: pinecone<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone) (6.0.2)\n",
            "Requirement already satisfied: langchain-tests<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (0.3.19)\n",
            "Requirement already satisfied: anthropic<1,>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from langchain-anthropic) (0.51.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.78.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.51.3)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.31.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.51.0->langchain-anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.51.0->langchain-anthropic) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.51.0->langchain-anthropic) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<1,>=0.51.0->langchain-anthropic) (1.3.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.3.5)\n",
            "Requirement already satisfied: pytest-asyncio<1,>=0.20 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.26.0)\n",
            "Requirement already satisfied: syrupy<5,>=4 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.9.1)\n",
            "Requirement already satisfied: pytest-socket<1,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.7.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.33.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.54b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone) (0.0.7)\n",
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.5.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install poppler-utils\n",
        "!sudo apt-get install tesseract-ocr\n",
        "!sudo apt-get install libtesseract-dev\n",
        "!pip install \"unstructured[all-docs]\" pillow pydantic lxml matplotlib unstructured-pytesseract\n",
        "!pip install chromadb grandalf langchain langchain-community langchain-groq langchain-pinecone pinecone-notebooks langchain-anthropic langchain-google-genai langchain-openai langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline, HuggingFaceEmbeddings\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "2wC4gIsi731X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MM Extraction"
      ],
      "metadata": {
        "id": "Aio96UDu4Ymn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1"
      ],
      "metadata": {
        "id": "kdcoj3Bq4dIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf"
      ],
      "metadata": {
        "id": "_3LC8fRCi83M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "def download_pdf(url, save_dir):\n",
        "    # Ensure the save directory exists\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Extract the filename from the URL\n",
        "    filename = url.split(\"/\")[-1]\n",
        "\n",
        "    # Construct the full path to save the file\n",
        "    file_path = os.path.join(save_dir, filename)\n",
        "\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "\n",
        "        # Write the content to the file in chunks\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "        print(f\"✅ File downloaded successfully: {file_path}\")\n",
        "\n",
        "        return file_path\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Failed to download file: {e}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "pdf_url = \"https://raw.githubusercontent.com/lokeshparab/GenAI-Full-Course/refs/heads/main/data/RAG_FOR_NLP.pdf\"\n",
        "destination_folder = \"data\"  # Replace with your desired directory path\n",
        "\n",
        "filename = download_pdf(pdf_url, destination_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMSUwSW72QNf",
        "outputId": "aa5d369c-abd3-45f8-f167-1470e0cc8eb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File downloaded successfully: data/RAG_FOR_NLP.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=filename,\n",
        "    strategy=\"hi_res\",\n",
        "    extract_images_in_pdf=True,\n",
        "    extract_image_block_types=[\"Image\",\"Table\"],\n",
        "    extract_image_block_to_payload=False,\n",
        "    extract_image_block_output_dir=\"extracted_data\",\n",
        "    infer_table_structure=True,\n",
        ")"
      ],
      "metadata": {
        "id": "-_IEupk3yjh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Header=[]\n",
        "Footer=[]\n",
        "Title=[]\n",
        "NarrativeText=[]\n",
        "Text=[]\n",
        "ListItem=[]\n",
        "Image=[]\n",
        "Table=[]\n",
        "for element in raw_pdf_elements:\n",
        "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
        "            Header.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
        "            Footer.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
        "            Title.append(str(element))\n",
        "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
        "            NarrativeText.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
        "            Text.append(str(element))\n",
        "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
        "            ListItem.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Image\" in str(type(element)):\n",
        "            Image.append(str(element))\n",
        "  elif \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "            Table.append(str(element))"
      ],
      "metadata": {
        "id": "HSk-UND01xeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Header"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPu2gy7Q4FQp",
        "outputId": "c914420b-455b-4b64-e276-703a68612570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2 r p A 2 1 ] L C . s c [ 4 v 1 0 4 1 1 . 5 0 0 2 :', '16', '19']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Title"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "019dcUrx4Gt0",
        "outputId": "4ec4d6ad-48a9-4553-ee2d-a4bb3e4f159b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['v',\n",
              " 'arXiv',\n",
              " 'i',\n",
              " 'X',\n",
              " 'r',\n",
              " 'a',\n",
              " 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks',\n",
              " 'Abstract',\n",
              " '1 Introduction',\n",
              " '2 Methods',\n",
              " '2.1 Models',\n",
              " 'N',\n",
              " '2.2 Retriever: DPR',\n",
              " 'py(z|x) x exp(d(z)\"q(x))',\n",
              " '2.3 Generator: BART',\n",
              " '2.4 Training',\n",
              " '2.5 Decoding',\n",
              " '3 Experiments',\n",
              " '3.1 Open-domain Question Answering',\n",
              " '3.2 Abstractive Question Answering',\n",
              " 'Jeopardy Question Generation',\n",
              " '3.4 Fact Veriﬁcation',\n",
              " '4 Results',\n",
              " '4.1 Open-domain Question Answering',\n",
              " 'Model',\n",
              " 'NQ',\n",
              " 'TQA WQ CT',\n",
              " 'Jeopardy MSMARCO FVR3 FVR2',\n",
              " '4.2 Abstractive Question Answering',\n",
              " 'Jeopardy Question Generation',\n",
              " '4.4 Fact Veriﬁcation',\n",
              " 'Doc 1 poe 2',\n",
              " 'Doc 3',\n",
              " '4.5 Additional Results',\n",
              " 'Table 5: Ratio of distinct to total tri-grams for',\n",
              " 'generation tasks.',\n",
              " 'FVR-2',\n",
              " 'Label Accuracy',\n",
              " '5 Related Work',\n",
              " '6 Discussion',\n",
              " 'Broader Impact',\n",
              " 'Acknowledgments',\n",
              " 'References',\n",
              " 'Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks',\n",
              " 'A Implementation Details',\n",
              " 'B Human Evaluation',\n",
              " 'C Training setup Details',\n",
              " 'D Further Details on Open-Domain QA',\n",
              " 'E Further Details on FEVER',\n",
              " 'F Null Document Probabilities',\n",
              " 'G Parameters',\n",
              " 'H Retrieval Collapse',\n",
              " 'I Number of instances per dataset']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NarrativeText"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA1QutAq4IRZ",
        "outputId": "037923b5-2719-40e6-8562-4b3d73c2e1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Patrick Lewis't, Ethan Perez*,\",\n",
              " 'Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,',\n",
              " 'Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†',\n",
              " '+Racebook AI Research; *University College London; *New York University;',\n",
              " 'plewis@fb.com',\n",
              " 'Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.',\n",
              " 'Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results,',\n",
              " 'Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.',\n",
              " 'but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.',\n",
              " 'We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.',\n",
              " 'There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.',\n",
              " 'Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera- tion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1',\n",
              " 'We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x,z,y1:i−1) parametrized',\n",
              " '1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/',\n",
              " 'by θ that generates a current token based on a context of the previous i − 1 tokens y1:i−1, the original input x and a retrieved passage z.',\n",
              " 'To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure.',\n",
              " 'RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized,',\n",
              " 'RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:',\n",
              " 'Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.',\n",
              " 'The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:',\n",
              " 'where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory.',\n",
              " 'The generator component pθ(yi|x,z,y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters θ as the parametric memory henceforth.',\n",
              " 'We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we',\n",
              " 'minimize the negative marginal log-likelihood of each target, ar',\n",
              " 'j −logp(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.',\n",
              " 'At test time, RAG-Sequence and RAG-Token require different ways to approximate argmaxy p(y|x).',\n",
              " 'RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera- tor with transition probability: pj (yi|z, yii—1) = zetop-k(p(-|2)) Pn (zil@)po(yil@, Zi, Y1e—1) To decode, we can plug Po(yi |x, y1i—1) into a standard beam decoder.',\n",
              " 'RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional per- token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using pθ(yi|x,z,y1:i−1). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with pη(z|x) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding, we can make a further approximation that pθ(y|x,zi) ≈ 0 where y was not generated during beam search from x,zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.”',\n",
              " 'We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k ∈ {5,10} for training and set k for test time using dev data. We now discuss experimental details for each task.',\n",
              " 'Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.',\n",
              " 'RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat',\n",
              " 'MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses.',\n",
              " 'To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task.',\n",
              " 'We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—quuestion A is better, question B is better, both are good, or neither is good.',\n",
              " 'FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.',\n",
              " 'Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance.',\n",
              " 'There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading',\n",
              " 'to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%.',\n",
              " 'As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5).',\n",
              " 'Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model.',\n",
              " 'Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together—the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.',\n",
              " 'Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.',\n",
              " 'Document 1: his works are considered classics of American literature ... His wartime experiences formed the basis for his novel “A Farewell to Arms” (1929) ... Document 2: ... artists of the 1920s “Lost Generation” expatriate Doe 4 community. His debut novel, \"The Sun Also Rises”, was published °° in 1926. Doc 5',\n",
              " '& , es ee £ te os & ss . TES eS',\n",
              " 'Figure 2: RAG-Token document posterior p(zi|x,yi,y−i) for each generated token for input “Hem- ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".',\n",
              " 'Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.',\n",
              " 'For 2-way classiﬁcation, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.',\n",
              " 'Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing any diversity-promoting decoding.',\n",
              " 'Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks.',\n",
              " 'We compare RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG’s retriever with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial.',\n",
              " 'Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed',\n",
              " 'Table 4: Human assessments for the Jeopardy Question Generation Task.',\n",
              " 'between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.',\n",
              " 'Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signiﬁcant differences in performance between them. We have the ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.',\n",
              " 'Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks.',\n",
              " 'General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench- marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed architecture, by learning a retrieval module to augment pre-trained, generative language models.',\n",
              " 'Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be ﬁne-tuned for strong performance on a variety of tasks.',\n",
              " 'Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9].',\n",
              " 'Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a ﬁnal output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work.',\n",
              " 'In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks.',\n",
              " 'This work offers several positive societal beneﬁts over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs.',\n",
              " 'With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against misleading content and automated spam/phishing.',\n",
              " 'The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program.',\n",
              " 'for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/ anthology/P19-1612.',\n",
              " 'approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_ 2016_paper9.pdf.',\n",
              " 'For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not ﬁnd beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.',\n",
              " 'Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\".',\n",
              " 'Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instruc- tions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.',\n",
              " 'We train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision ﬂoating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We ﬁnd that doing Maximum Inner Product Search with FAISS is sufﬁciently fast on CPU, so we store document index vectors on CPU, requiring ∼ 100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66]3, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS’s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/ blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/',\n",
              " '2https://github.com/pytorch/fairseq 3https://github.com/huggingface/transformers',\n",
              " 'For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to ﬁnd matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q,a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we ﬁlter out answer candidates if they do not occur in top 1000 documents for the query.',\n",
              " 'CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expres- sions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we ﬁrst retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.',\n",
              " 'TriviaQA Evaluation setups The open-domain QA community customarily uses public develop- ment datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading compehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA ofﬁcial Wikipedia test set instead. Févry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We ﬁnd that our performance is much higher using the ofﬁcial Wiki test set, rather than the more conventional open-domain test set, which we attribute to the ofﬁcial Wiki test set questions being simpler to answer from Wikipedia.',\n",
              " 'For FEVER classiﬁcation, we follow the practice from [32], and ﬁrst re-generate the claim, and then classify using the representation of the ﬁnal hidden state, before ﬁnally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The ﬁrst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER’s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classiﬁcation prediction. As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work.',\n",
              " 'We experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not ﬁnd that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to beneﬁt from retrieval, suggesting that null document mechanisms may not be necessary for RAG.',\n",
              " 'Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable',\n",
              " 'parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non- parametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating point precision to manage memory and disk footprints.',\n",
              " 'In preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would “collapse” and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.',\n",
              " 'The number of training, development and test datapoints in each of our datasets is shown in Table 7.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6vPufy04K94",
        "outputId": "dc3b8804-0e76-4cb2-9873-f802ce0084c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1', '2021', '2', '0', '92.2*', '6', '9']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6XnXCx64PZG",
        "outputId": "cc9b1c8a-1262-4719-ed92-4568d316fd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Closed Book T5-11B [52] 34.5 T5-11B+SSM[52] 36.6 - - /50.1 37.4 /60.5 44.7 - - Model B-1 QB-1 R-L B-1 Label Acc. Open REALM [20] 40.4 - / - 40.7 46.8 SotA - - 49.8* 49.9* 76.8 Book DPR [26] 41.5 57.9/ - 41.1 50.6 BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Token RAG-Seq. 44.1 55.2/66.1 45.5 50.0 44.5 56.8/68.0 45.2 52.2 RAG-Tok. 17.3 22.2 RAG-Seq. 14.7 21.4 40.1 40.8 41.5 44.2 72.5 89.5',\n",
              " 'Task Input Model Generation MS- deﬁne middle ear BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. MARCO what currency BART The currency needed in Scotland is Pound sterling. needed in RAG-T Pound is the currency needed in Scotland. scotland RAG-S The currency needed in Scotland is the pound sterling. BART ?This state has the largest number of counties in the U.S. Jeopardy Washington RAG-T It’s the only U.S. state named for a U.S. president Question RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park Gener -ation The Divine Comedy BART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio RAG-T Dante’s \"Inferno\" is the ﬁrst part of this epic poem RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"',\n",
              " 'Factuality Speciﬁcity MSMARCO Jeopardy QGen BART better RAG better Both good Both poor No majority 7.1% 42.7% 11.7% 17.7% 20.8% 16.8% 37.4% 11.8% 6.9% 20.1% Gold BART RAG-Token RAG-Seq. 89.6% 70.7% 77.8% 83.5% 90.0% 32.4% 46.8% 53.8%',\n",
              " 'Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 Exact Match B-1 QB-1 R-L B-1 RAG-Token-BM25 RAG-Sequence-BM25 29.7 31.8 41.5 44.1 32.1 36.6 33.1 33.8 17.5 11.1 22.3 19.5 55.5 56.5 48.4 46.9 75.1 91.6 RAG-Token-Frozen RAG-Sequence-Frozen 37.8 41.2 50.1 52.1 37.1 41.8 51.1 52.6 16.7 11.8 21.7 19.6 55.9 56.7 49.4 47.3 72.9 89.4 RAG-Token RAG-Sequence 43.5 44.0 54.8 55.8 46.5 44.9 51.9 53.4 17.9 15.3 22.6 21.5 56.2 57.2 49.4 47.5 74.5 90.6',\n",
              " 'Task Train Development Test Natural Questions 79169 8758 3611 TriviaQA 78786 8838 11314 WebQuestions 3418 362 2033 CuratedTrec 635 134 635 Jeopardy Question Generation 97392 13714 26849 MS-MARCO 153726 12468 101093* FEVER-3-way 145450 10000 10000 FEVER-2-way 96966 6666 6666']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzq_u80_4Q-g",
        "outputId": "ba46b059-776c-431e-94bd-e177bfc49944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@--- ee ee ee ee ee ee ee ee ee ee eee The middle ear includes End-to-End Backprop through q and pe the tympanic cavity and the three ossicles. (y) Define \"middle ear\" (x) Question Answering: Question Query Retriever py Document Generator pg “fnower Generation Index. (Non-Parametric) (Parametric) d(z) supports (y) Barack Obama was born in Hawaii. (x) q(x) Fact Verification: Fact Query Fact Verification: Label Generation The Divine This 14th century work Comedy (x) is divided into 3 Jeopardy Question Generation: Answer Query sections: \"Inferno\", \"purgatorio\" & \"Paradiso\" @) Question Generation',\n",
              " '| | | |',\n",
              " 'Bee TT % 80 Porm Sa SRS nana ga g / Z fr = 70 2 | / 3 RAG TORRE 2 nf |! g <= RAG-Tok B-1 Ba Ze H=- RAGSeq RL a ; 3 Zs == RAG-Seq BA Q 50 > 50 ZO — reactor | & 3 soft === RAGSeq | Z 40 2 4s rr rr rr nr) rr nr K Retrieved Docs K Retrieved Docs K Retrieved Docs',\n",
              " 'View full instructions Which sentence is more factually true? View tool guide Select an option Subject : Hemingway eI Note: Some questions are Sentence Ais more 1 control questions. We require Sentence A : \"The Sun Also Rises\" is a novel by this author of \"A true good accuracy on our control Farewell to Arms\" Sentence Bis more 2 questions to accept true responses. Sentence B : This author of \"The Sun Also Rises\" was born in Both sentences are 8 Havana, Cuba, the son of Spanish immigrants ‘rue Indicate which one of the P a following sentences is more Both sentences are factually true with respect to completely untrue the subject. Using the internet to check whether the sentences are true is encouraged.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model loading"
      ],
      "metadata": {
        "id": "DSHB5G2k8boG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Model"
      ],
      "metadata": {
        "id": "D_sW3-Ei79mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    # repo_id = \"perplexity-ai/r1-1776\",\n",
        "    task=\"text-generation\"\n",
        "  )\n",
        "# llm=HuggingFaceEndpoint(repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",task=\"text-generation\")\n",
        "hf_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "try:\n",
        "  hf_model.invoke(\"Hi I am Lokesh\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "yab7K2EePSsd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b5c636-1ef6-4abb-f2df-fdf2da1399e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
        "gemini_model.invoke(\"Hi I am Lokesh\")"
      ],
      "metadata": {
        "id": "Fx01kgSQTeTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1eda044-0ef2-4cd7-b35b-2a22550c01e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello Lokesh!  How can I help you today?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--37a5e30c-8d2f-4cdc-8552-d43bf1687762-0', usage_metadata={'input_tokens': 5, 'output_tokens': 13, 'total_tokens': 18, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_model = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
        "groq_model.invoke(\"Hi I am Lokesh\")"
      ],
      "metadata": {
        "id": "zWMcWKv8eCQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d5118d-bd2f-4b7f-c3a8-d84c9e126678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='<think>\\n\\n</think>\\n\\nHi Lokesh! How can I assist you today? 😊', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 8, 'total_tokens': 26, 'completion_time': 0.100251449, 'prompt_time': 0.000162046, 'queue_time': 0.206830667, 'total_time': 0.100413495}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_1bbe7845ec', 'finish_reason': 'stop', 'logprobs': None}, id='run--2f4b3146-8041-4aa7-804b-01ec6ab0e064-0', usage_metadata={'input_tokens': 8, 'output_tokens': 18, 'total_tokens': 26})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open_ai_model = ChatOpenAI(model=\"gpt-4o\")\n",
        "open_ai_model.invoke(\"Hi I am Sunny\")"
      ],
      "metadata": {
        "id": "k0O0bFZxeQrT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d294a112-5068-4281-b2ba-a28f817aa4e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello Sunny! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_55d88aaf2f', 'id': 'chatcmpl-BX7mPDFSz18yDIGLmS3V2f3Zo5IbS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f8fba0e1-e6f3-4bd6-ba18-05afe1209319-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "claude_model = ChatAnthropic(model=\"claude-2\")\n",
        "claude_model.invoke(\"Hi I am Sunny\")"
      ],
      "metadata": {
        "id": "fekGPHO0bUG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db279a69-ba18-4f83-d6e8-70e6b161c4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello Sunny, nice to meet you!', additional_kwargs={}, response_metadata={'id': 'msg_016uaA299mfgGchcZWYUk5NJ', 'model': 'claude-2', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 14, 'output_tokens': 13, 'server_tool_use': None}, 'model_name': 'claude-2'}, id='run--6d07ddd4-3aec-469f-9477-a3d78baa395e-0', usage_metadata={'input_tokens': 14, 'output_tokens': 13, 'total_tokens': 27, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "    task='text-generation',\n",
        "    pipeline_kwargs={\n",
        "        \"temperature\": 0.5,\n",
        "        \"max_new_tokens\":100\n",
        "        }\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "model.invoke(\"Hi I am Lokesh\")"
      ],
      "metadata": {
        "id": "IgmG5tFwecH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embeding Model"
      ],
      "metadata": {
        "id": "tMWLHLX98BmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # OpenAI\n",
        "\n",
        "embedding_model = \"text-embedding-3-large\" # @param [\"text-embedding-3-large\",\"text-embedding-3-small\",\"text-embedding-ada-002\"]\n",
        "dimensions = 64 #@param {type:\"integer\"}\n",
        "set_dimension = True # @param {type:\"boolean\"}\n",
        "query = \"India is a growing country\" # @param {\"type\":\"string\",\"placeholder\":\"India is a growing country\"}\n",
        "\n",
        "if set_dimension:\n",
        "  openai_embedding = OpenAIEmbeddings(\n",
        "      model=embedding_model,\n",
        "      dimensions=dimensions,\n",
        "  )\n",
        "else:\n",
        "  openai_embedding = OpenAIEmbeddings(\n",
        "      model=embedding_model,\n",
        "  )\n",
        "\n",
        "result = openai_embedding.embed_query(query)\n",
        "print(len(result),result)"
      ],
      "metadata": {
        "id": "EMOwYZgO8DuK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1abd2c2e-1646-45fb-f77c-945b9daa04f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64 [-0.22190459072589874, 0.2781512439250946, -0.019638171419501305, 0.047116201370954514, 0.007093434687703848, 0.0018865261226892471, -0.15810702741146088, 0.09076514095067978, 0.03236108645796776, -0.12227867543697357, 0.08452407270669937, -0.04599897190928459, 0.11403430253267288, -0.10378662496805191, -0.10417187958955765, 0.014938108623027802, 0.08899299055337906, -0.20634044706821442, -0.0308393444865942, -0.1088719367980957, -0.19863542914390564, -0.11588350683450699, 0.045228470116853714, -0.17860238254070282, -0.21543237566947937, 0.09045694023370743, -0.036541059613227844, 0.003484114073216915, -0.15533322095870972, 0.13183289766311646, 0.09477175027132034, 0.014052031561732292, 0.06156311556696892, -0.05451301857829094, -0.03172542154788971, 0.27722662687301636, -0.024501964449882507, -0.054859746247529984, -0.07520099729299545, -0.08383062481880188, -0.007314953953027725, 0.015670085325837135, -0.11287855356931686, 0.4059004783630371, 0.1887730062007904, 0.010604034177958965, -0.02933686599135399, -0.14585603773593903, -0.08583392947912216, -0.020533880218863487, -0.25580668449401855, 0.09037989377975464, -0.027930699288845062, 0.05015968531370163, -0.11064409464597702, 0.03607875853776932, -0.014235025271773338, 0.08845363557338715, -0.03933412954211235, -0.16026443243026733, -0.10833258926868439, -0.0478096529841423, -0.017278509214520454, 0.19031400978565216]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Google GenAi\n",
        "\n",
        "embedding_model = \"models/gemini-embedding-exp-03-07\" # @param [\"models/gemini-embedding-exp-03-07\",\"models/text-embedding-004\",\"models/embedding-001\"]\n",
        "task_type = \"retrieval_query\" # @param [\"None\",\"task_type_unspecified\",\"retrieval_query\",\"retrieval_document\",\"semantic_similarity\",\"classification\",\"clustering\"]\n",
        "transport = \"None\" # @param [\"None\",\"rest\",\"grpc\",\"grpc_asyncio\"]\n",
        "query = \"India is a growing country\" # @param {\"type\":\"string\",\"placeholder\":\"India is a growing country\"}\n",
        "\n",
        "func = lambda x : None if x==\"None\" else x\n",
        "task_type = func(task_type)\n",
        "transport = func(transport)\n",
        "\n",
        "google_embedding = GoogleGenerativeAIEmbeddings(\n",
        "    model=embedding_model,\n",
        "    task_type=task_type,\n",
        "    transport=transport\n",
        ")\n",
        "\n",
        "result = google_embedding.embed_query(query)\n",
        "print(len(result),result)\n"
      ],
      "metadata": {
        "id": "z0xAC56Z8QEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c90942-97aa-46de-88b7-7dec93b6098c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3072 [-0.01328858733177185, -0.008076989091932774, -0.02803654596209526, -0.0385795421898365, -0.0012177267344668508, -0.009753282181918621, 0.018366066738963127, 0.0390302836894989, 0.005043548997491598, -0.023734668269753456, -0.011821064166724682, 0.006893169600516558, 0.019229836761951447, 0.03314683958888054, 0.11351391673088074, 0.020717905834317207, 0.0073205651715397835, -0.019075468182563782, 0.006459957454353571, -0.0205469261854887, -0.003892261302098632, 0.01249883696436882, 0.01834138110280037, 0.004717701114714146, -0.005373682361096144, -0.0032776377629488707, 0.006898732855916023, 0.02193414606153965, 0.021265285089612007, 0.014482101425528526, 0.005145769566297531, -0.027150483801960945, 0.023643454536795616, 0.012725071050226688, 0.017535502091050148, 0.015545469708740711, 0.022201204672455788, 0.0026000298094004393, 0.02127012610435486, 0.005257639102637768, -0.02616986259818077, 0.009095565415918827, -0.022166412323713303, -0.007171641103923321, -0.028205083683133125, -0.009816399775445461, -0.013868274167180061, -0.0021237717010080814, -0.00363388704136014, 0.014518032781779766, -0.008481813594698906, -0.0129407849162817, -0.030563408508896828, -0.16161364316940308, -0.00013045463128946722, 0.010254891589283943, -0.003525951411575079, -0.005071470979601145, 0.02015729807317257, 0.000320611143251881, -0.008248499594628811, 0.01632642187178135, -0.0036300814244896173, 0.013907567597925663, 0.002236715517938137, -0.012520856224000454, 0.014694290235638618, -0.02716130204498768, -0.0030767086427658796, 0.017162470147013664, 0.03978263586759567, 0.008536068722605705, 0.011873491108417511, -0.0011896451469510794, -0.007652594242244959, -0.007194088771939278, 0.0008688752423040569, 0.010223313234746456, -0.006604047026485205, 0.05249914154410362, -0.02720547467470169, -0.008542787283658981, -0.008755945600569248, -0.0224667526781559, -0.007026237901300192, -0.023030972108244896, -0.016797348856925964, -0.014020211063325405, -0.026935499161481857, -0.0015378773678094149, 0.002020171843469143, 0.003660211805254221, 0.000308151647914201, -0.004498661961406469, -0.010725809261202812, 0.027641596272587776, -0.039983220398426056, 0.008669508621096611, 0.02008422464132309, -0.0029375043231993914, 0.016934974119067192, -0.006346066482365131, 0.011027980595827103, -0.006908406503498554, -0.008716331794857979, 0.007638164795935154, 0.013640833087265491, 0.00021102986647747457, 0.02662036009132862, 0.02476804330945015, 0.0017494743224233389, 0.0012057898566126823, -0.010028357617557049, 0.020040303468704224, -0.005316232331097126, -0.1729799509048462, -0.001118279411457479, 0.023696739226579666, 0.009850597940385342, 0.012775289826095104, 0.0015446918550878763, 0.009080201387405396, -0.004664798732846975, 0.021889563649892807, 0.0021150754764676094, 0.009001697413623333, 0.005126600153744221, 0.025885142385959625, -0.01286401692777872, 0.0215202234685421, -0.013138740323483944, -0.004653527867048979, -0.007690559606999159, -0.002446215832605958, 0.009388744831085205, 0.017057884484529495, 0.02007322944700718, -0.01612553559243679, 0.01779586263000965, -0.04356278106570244, -0.005822404287755489, 0.00584682310000062, -0.010354278609156609, -0.003033964429050684, 0.014901647344231606, -0.026056863367557526, 0.010833218693733215, -0.003392668440937996, 0.01776634156703949, -0.004775885958224535, 0.013060222379863262, 0.01801242120563984, 0.022511370480060577, 0.005569703411310911, -0.016939904540777206, -0.015604627318680286, 0.011984610930085182, 0.006109213922172785, 0.0007568472647108138, 0.0007346800994127989, 1.3178240806155372e-05, -0.005773508921265602, -0.0057408371940255165, 0.029451772570610046, 0.009471258148550987, 0.00675201416015625, -0.035964950919151306, 0.027263598516583443, 0.030009020119905472, 0.011075926944613457, -0.009842092171311378, -0.004086132626980543, 0.013223040848970413, 0.013967524282634258, -0.005430908408015966, -0.00701926788315177, -0.002991889137774706, 0.011471616104245186, 0.012308084405958652, -0.017933743074536324, -0.007461375091224909, -0.007117716129869223, 0.0061567616648972034, 0.004008299671113491, 0.027051839977502823, -0.014606441371142864, -0.006023608148097992, 0.024581369012594223, -0.005047841463238001, 0.017041459679603577, 0.009725897572934628, -0.012158689089119434, -0.010167291387915611, 0.01833183690905571, 0.008918344974517822, -0.007445288356393576, -0.004829488694667816, -0.005451180972158909, 0.021369358524680138, -0.010841966606676579, 0.004771878477185965, -0.01536942832171917, 0.0019211956532672048, -0.040554966777563095, 0.02200292982161045, -0.00141267164144665, -0.0030087961349636316, 0.011986348778009415, -0.005316930823028088, 0.003230549395084381, 0.011558122001588345, -0.016957156360149384, 0.0032684672623872757, -0.02911391295492649, 0.010184552520513535, 0.028007997199892998, 0.019913680851459503, -0.003779997583478689, 0.007493571378290653, -0.0032305074855685234, 0.004747293423861265, -0.0006689053261652589, -0.017515406012535095, -0.007237907964736223, -0.03203301504254341, 0.004186207428574562, 0.007935373112559319, 0.004376985598355532, -0.0072417110204696655, -0.005618639290332794, 0.0005628494545817375, -0.01893950067460537, -0.008714668452739716, 0.021415019407868385, 0.007415829226374626, 0.01574568636715412, 0.0020907940343022346, 0.004538604058325291, -0.005613327957689762, 0.005402945913374424, -0.0005990678328089416, 0.0096841249614954, -0.0005873253685422242, -0.007221763487905264, -0.01987713761627674, 0.009963876567780972, 0.0102542070671916, -0.011801251210272312, -0.004612331744283438, -0.0033667890820652246, -0.017371155321598053, -0.005380946211516857, 0.0019039734033867717, -0.013756928965449333, -0.013157353736460209, -0.007344068493694067, 0.00641285814344883, -0.006528493016958237, 0.013233501464128494, -0.010545504279434681, 0.03203568235039711, 0.00027929586940445006, -0.011440623551607132, 0.010218425653874874, -0.02321087196469307, 0.009720535948872566, -0.018735071644186974, 0.0008710568654350936, -0.0058290595188736916, 0.003551002824679017, 0.02016921155154705, 0.0010876379674300551, -0.027063217014074326, 0.005553065333515406, 0.006464877165853977, 0.00749899772927165, -0.10462263971567154, -0.017483368515968323, -0.015061816200613976, -0.018675850704312325, -0.008898206055164337, 0.006106674205511808, -0.00027126900386065245, -0.009967370890080929, 0.013854856602847576, -0.0028792161028832197, 0.004857202991843224, -0.03870219364762306, 0.02305835485458374, -0.001044764881953597, 0.0026640358846634626, -0.003949160221964121, 0.011039276607334614, -0.02238893322646618, 0.019215410575270653, -0.003433150239288807, 0.008415354415774345, -0.024734986945986748, -0.016157107427716255, -0.033389583230018616, 0.0015876912511885166, -0.02166362851858139, 0.006787093356251717, 0.02423846535384655, 0.012233495712280273, 0.007516331505030394, 0.003762052161619067, 0.001421425142325461, -0.008626443333923817, -0.017286352813243866, 0.0005761257489211857, 0.016438130289316177, 0.005258513614535332, -0.002808360615745187, 0.011932657100260258, -0.0015924813924357295, -0.01784755289554596, -0.0008568678167648613, 0.0058567882515490055, 0.007032514549791813, -0.01029953546822071, 0.0068122027441859245, 0.016625018790364265, 0.005658837500959635, -0.03484586253762245, 0.004347261041402817, -0.009011661633849144, 0.014271852560341358, 0.01746724359691143, -0.027600325644016266, -0.011917353607714176, -0.019400212913751602, 0.013688918203115463, -0.012054309248924255, -0.015026874840259552, 0.02563525177538395, 0.0207963977009058, 0.012762758880853653, 0.025196097791194916, 0.022890936583280563, -0.0034276004880666733, -0.007387765683233738, 0.01176832988858223, 0.004306682851165533, 0.018149549141526222, -0.014347770251333714, 0.0011797891929745674, -0.03418947011232376, -0.015698017552495003, -0.02022680640220642, 0.020329689607024193, 0.026210149750113487, -0.007637994829565287, -0.004439729265868664, -0.01937456801533699, 0.02001914568245411, 0.012015653774142265, -0.00273223128169775, -0.0171341672539711, -0.029504980891942978, 0.010965420864522457, -0.0026978219393640757, -0.004007942508906126, 0.013661948032677174, 0.011280725710093975, -0.027947017922997475, 0.0026866968255490065, -0.0044016302563250065, 0.005154501646757126, 0.021115565672516823, 0.012378512881696224, -0.03470443934202194, 0.002687624655663967, 0.008444366045296192, 0.01721627078950405, -0.004192827735096216, 0.028254201635718346, 0.0047379727475345135, 0.004425984341651201, -0.016044648364186287, -0.009902129881083965, 0.004982952959835529, 0.001839779200963676, 0.002410158747807145, -0.025146108120679855, 0.0039494093507528305, 0.018323805183172226, -6.331028998829424e-05, -0.02706882171332836, 0.00947005394846201, -0.015696292743086815, 0.005258192308247089, 0.001486025983467698, 0.0074331071227788925, -0.00814251508563757, -0.0019101533107459545, -0.0008601164445281029, -0.014394747093319893, -0.024475831538438797, -0.010162217542529106, -0.011837058700621128, 0.00260562589392066, -0.006542458664625883, 0.0018276824848726392, 0.02373342774808407, -0.012077194638550282, -0.013622510246932507, -0.01414803508669138, -0.009060469456017017, 0.014645912684500217, 0.019193386659026146, -0.010457796044647694, 0.02766517549753189, -4.307894050725736e-05, 0.0081464983522892, 0.014720146544277668, -0.0013719691196456552, -0.0003253570175729692, -0.0038764376658946276, -0.0010302538285031915, 0.006728939712047577, -0.003243714338168502, 0.0037410028744488955, 0.02397567592561245, -0.036909107118844986, -0.052270159125328064, -0.0050603775307536125, -0.016918903216719627, 0.0013149197911843657, 0.01762070320546627, -0.014276213012635708, -0.016231177374720573, -0.0239238478243351, 0.010637004859745502, 0.024114936590194702, -0.000958852528128773, -0.011494380421936512, 0.006723067257553339, -0.017161644995212555, -0.021848348900675774, 0.012520384043455124, 0.03133997321128845, 0.005922100506722927, -0.0010543587850406766, 0.011450430378317833, 0.004480584058910608, -0.026533450931310654, 0.00996390450745821, 0.0066037867218256, -0.012916270643472672, -0.010910965502262115, 0.0066541098058223724, 0.002415962517261505, -0.008300118148326874, -0.00542700057849288, -0.0036218289751559496, -0.022138280794024467, 0.008680958300828934, -0.016180770471692085, 0.005901570897549391, -0.009499143809080124, -0.03682234510779381, -0.003500863444060087, 0.014422590844333172, -0.005550962407141924, 0.0009487607167102396, 0.009545882232487202, -0.006882570218294859, 0.011903900653123856, 0.028826549649238586, -0.0013902389910072088, 0.00026085827266797423, -0.011553093791007996, -0.018907060846686363, 0.012374537996947765, -0.006712971720844507, 0.004027099814265966, -0.012835266068577766, 0.011239612475037575, -0.02201913297176361, -0.003912024199962616, 0.02954147756099701, 0.03763280436396599, -0.017041103914380074, 0.0014771586284041405, 0.030036956071853638, -0.0148527966812253, 0.010300871916115284, -0.009595811367034912, 0.02844923920929432, 0.010783259756863117, 0.04309343546628952, -0.00353330303914845, 0.022411255165934563, 0.014991506934165955, -0.004550333134829998, -0.020099451765418053, -0.03624674305319786, 0.0007640434196218848, 0.008924929425120354, 0.02260611020028591, 0.006523012183606625, 0.007508858572691679, -0.01289471797645092, 0.0037648987490683794, 0.0010571889579296112, 0.01962236315011978, 0.014318239875137806, -0.006821987219154835, -0.004595195408910513, -0.03598346933722496, 0.007755048107355833, -0.0049430252984166145, 0.01730780489742756, -0.02996545284986496, -0.009809802286326885, -0.03953634575009346, -0.021993208676576614, 0.01781483180820942, -0.004618403967469931, 0.00497483229264617, -0.0037403141614049673, -0.0024006948806345463, -0.014433327130973339, -0.0021481961011886597, 0.02964198589324951, 0.014213464222848415, 0.014412649907171726, 0.011648689396679401, 0.0010990963783115149, -0.01020006276667118, 0.047519877552986145, 0.004143726546317339, 0.01015120092779398, -0.011310561560094357, -0.0003509059315547347, 0.0054963380098342896, -0.022703003138303757, 0.016693342477083206, 0.013723064213991165, 0.0014658075524494052, -0.005842177662998438, -0.017931153997778893, 0.0029256592970341444, 0.0360502228140831, -0.11751149594783783, -0.005792273208498955, 0.025081578642129898, -0.025802306830883026, -0.016527732834219933, 0.009331448934972286, 0.04134884849190712, -0.02390679158270359, 0.020924411714076996, -0.005896535702049732, -0.008752789348363876, -0.0033869019243866205, 0.009435515850782394, -0.0219638179987669, -0.003517063334584236, 0.009109681472182274, 0.020551074296236038, 0.02668928913772106, -0.0013479777844622731, -0.021931542083621025, 0.014413649216294289, 0.02952062524855137, 0.007464569061994553, 0.01369615737348795, -0.010624933056533337, 0.021618660539388657, -0.00940257403999567, 0.014302127063274384, -0.040469951927661896, -0.02712346985936165, -0.017977075651288033, -0.02341991662979126, -0.0030013860668987036, 0.009440809488296509, 0.02800152264535427, -0.0258029792457819, 0.003907973878085613, 0.024756737053394318, 0.004653310403227806, -0.0030385886784642935, 0.02321702428162098, 0.01377109158784151, -0.0071786558255553246, -0.006920694839209318, -0.009031925350427628, -0.0014905947027727962, 0.015182940289378166, -0.0023227070923894644, -0.0076354509219527245, 0.02026692032814026, -0.03693917766213417, -0.02441369742155075, 0.01564507931470871, 0.0018958266591653228, -0.007500235922634602, -0.017256397753953934, -0.010287887416779995, 0.01717860996723175, 0.010272026993334293, -0.002609944436699152, -0.01589532569050789, 0.026556087657809258, 0.01207673829048872, 0.01922982558608055, 0.012770542874932289, -0.0102620804682374, -0.019737670198082924, 0.0039646257646381855, 0.029812529683113098, 0.014455199241638184, -0.00563602801412344, 0.0011948972241953015, -0.024004636332392693, 0.009614778682589531, 0.013962561264634132, 0.00906507670879364, 0.04832981526851654, 0.0146405640989542, 0.015451514162123203, -0.010362319648265839, -0.012805408798158169, 0.00021689539426006377, -0.0750727504491806, -0.01418739091604948, -0.007728958502411842, -0.010442612692713737, 0.029987875372171402, -0.011085128411650658, 0.02498294971883297, -0.0018163984641432762, -0.020261796191334724, 0.041012950241565704, 0.01655394956469536, 0.008147467859089375, 0.003376768436282873, -0.018934354186058044, 0.003468253882601857, -0.011688867583870888, 0.04292931780219078, -0.006761870812624693, -0.03044573776423931, 0.002786266850307584, -0.010424476116895676, -0.006910531781613827, -0.002776909153908491, 0.01617816649377346, 0.010964591056108475, 0.01573224924504757, 0.01834448240697384, -0.0034775889944285154, -0.005235271994024515, -0.01611205004155636, -0.005497316364198923, -0.12945277988910675, -0.0033692819997668266, -0.009314854629337788, -0.017597265541553497, 0.015999484807252884, 0.009192251600325108, 0.019983192905783653, -0.017605412751436234, -0.03313663974404335, -0.01994767226278782, 0.018786055967211723, -0.007588492706418037, -0.03689134493470192, -0.005885520949959755, 0.03926307335495949, 0.1315392255783081, -0.014131598174571991, 0.014688683673739433, -0.00018412916688248515, -0.026730824261903763, 0.0016901098424568772, 0.005101846996694803, 0.004284312948584557, -0.01493796519935131, -0.005246900487691164, 0.007614889647811651, 0.005465996451675892, 0.00252100289799273, 0.013686633668839931, -0.002985991770401597, 0.008780748583376408, -0.008530582301318645, 0.0066359457559883595, 0.0076441578567028046, -0.008641986176371574, 0.013642092235386372, 0.01027417741715908, 0.010010742582380772, 0.01780262216925621, -0.023556148633360863, 0.035923559218645096, 0.0013729817001149058, -0.026909608393907547, 0.000373545044567436, -0.014378062449395657, -0.01292633917182684, 0.0056676375679671764, -0.028638223186135292, -0.004468465689569712, 0.021307380869984627, -0.01082415971904993, -0.07938436418771744, -0.001620759954676032, 0.028142979368567467, -0.0009418439585715532, -0.018306566402316093, 0.005071737337857485, 0.004331565462052822, -0.017061475664377213, 0.012779648415744305, -0.00032267061760649085, -0.01813609153032303, -0.00809529609978199, -0.00035403616493567824, -0.027980050072073936, 0.01461845450103283, 0.022495754063129425, -0.005554698407649994, 0.0036493507213890553, 0.0298948772251606, -0.019249850884079933, 0.0028817576821893454, -0.0015675564063712955, 0.027793988585472107, 0.015440907329320908, -0.018435392528772354, -0.004554152954369783, 0.026539746671915054, 0.008480986580252647, -0.0023713356349617243, -0.020937299355864525, -0.00037827150663360953, 0.028483670204877853, 0.0063894749619066715, 0.02249014750123024, -0.04134916886687279, -0.005675296299159527, 0.008896609768271446, -0.0019385573687031865, 0.01748812384903431, -0.014470286667346954, -0.008413533680140972, -0.008721327409148216, -0.0004697235708590597, 0.011241680011153221, -0.0116786053404212, -0.026882072910666466, 0.017150288447737694, 0.035568952560424805, -0.019255181774497032, 0.03189807012677193, -0.01572776958346367, 0.03019873984158039, -0.03333519399166107, -0.01474298257380724, 0.0016041190829128027, 0.013395769521594048, -0.010776839219033718, -0.010052177123725414, 0.008153068833053112, -0.0017413757741451263, -0.02201179414987564, -0.025120873004198074, 0.016354944556951523, -0.006429646164178848, 0.009535226970911026, 0.007025949656963348, 0.011537018232047558, 0.010505151003599167, -0.010308469645678997, 0.012192036025226116, 0.025505565106868744, -0.014862403273582458, -0.0034718059469014406, 0.0014514017384499311, 0.009804384782910347, -0.006480119191110134, 0.004014854319393635, 0.0069696372374892235, -0.014871803112328053, -0.003626797581091523, 0.01152511965483427, 0.00015950424131006002, 0.0019964927341789007, 0.016392502933740616, 0.0141539191827178, -0.007166753988713026, -0.005720288958400488, -0.007052588742226362, -0.009472386911511421, 0.026133915409445763, 0.00022589077707380056, -0.0024966213386505842, 0.001363577088341117, 0.010817380622029305, 0.007715876214206219, 4.631416595657356e-05, -0.0012967473594471812, 0.008814670145511627, -0.005072040483355522, 0.007134543266147375, -0.011244363151490688, 0.013078015297651291, 0.0060137612745165825, 0.0143817700445652, 0.017532138153910637, 0.009082538075745106, 0.02732601761817932, -0.0007949123973958194, 0.0004999272641725838, 0.00247476645745337, -0.0014520756667479873, -0.006443042773753405, -0.005626296158879995, -0.001705923699773848, 0.002901969477534294, -0.010938704945147038, -0.0017223182367160916, 0.009410458616912365, 0.02929300256073475, 0.013756281696259975, 0.0064602321945130825, -0.02961556985974312, -0.021045314148068428, -0.007610950153321028, 0.007160964421927929, -0.006207238417118788, -0.011247849091887474, -0.015818139538168907, 0.006418820004910231, -0.0030257808975875378, -0.004327889531850815, -0.00033444419386796653, 0.015929915010929108, -0.016812840476632118, 0.02405374124646187, 0.0300450399518013, 0.006558182183653116, 0.015038037672638893, 0.009853418916463852, 0.0028953973669558764, -0.011019737459719181, 0.023617299273610115, 0.0009789341129362583, -0.001037341309711337, 0.013233627192676067, -0.010271476581692696, -0.009546296671032906, -0.0003561167686711997, 0.01560923084616661, 0.009693445637822151, -0.001438385690562427, 0.011524706147611141, -0.006340225227177143, -0.001619722810573876, 0.011298262514173985, 0.0015459766145795584, -0.008991770446300507, 0.00025864929193630815, 0.016127850860357285, 0.003312622429803014, 0.010512317530810833, -0.007343323901295662, 0.008342706598341465, -0.021729014813899994, -0.00474186334758997, -0.01970616541802883, -0.004252954386174679, -0.0005697408341802657, -0.008934436365962029, -0.002213083440437913, -0.0019178080838173628, 0.004072417505085468, -0.012840895913541317, -0.005146440584212542, -0.013458194211125374, -6.72814276185818e-05, -0.010060150176286697, 0.008334633894264698, 0.0017405423568561673, -0.0026581967249512672, -0.0021273954771459103, -0.00423845648765564, 0.010107554495334625, -0.003482712898403406, 0.0009100022143684328, 0.010695471428334713, -0.0020791247952729464, 0.013637256808578968, -0.01229273620992899, 0.009968052618205547, -0.008636167272925377, 0.004983231890946627, -0.0014730397379025817, 0.0018485183827579021, -0.002666736952960491, 0.016015559434890747, 0.018517211079597473, -0.019358286634087563, 0.02471931092441082, -0.0017484348500147462, 0.014403392560780048, -0.0033955792896449566, 0.005579076707363129, -0.019219741225242615, 0.002039544750005007, -0.003054292406886816, 0.012830546125769615, -0.016909072175621986, 0.008328012190759182, -0.00444420799612999, 0.010171854868531227, 0.0019433186389505863, 0.02751610428094864, -0.003942765295505524, -0.012731576338410378, -0.00979330763220787, -0.012039054185152054, -0.006642154883593321, 0.014390452764928341, 0.01548704318702221, 0.004574477206915617, 0.002438075141981244, 0.009177112951874733, 0.0010290315840393305, -0.0030991709791123867, -0.0007719369023106992, -0.0010907496325671673, 0.0042881774716079235, -0.0041784681379795074, 0.0019744005985558033, -0.014964361675083637, 2.162704367947299e-05, 0.010843265801668167, -0.006398896221071482, -0.012601200491189957, 0.005770224146544933, -0.0006175790913403034, 0.017544828355312347, -0.01852460764348507, -0.00915355235338211, -0.002896183868870139, -0.013982756994664669, 0.009359491989016533, -0.0019762939773499966, 0.0039041407871991396, 0.004062795080244541, 0.005871545523405075, -0.010342412628233433, -0.005209058057516813, 0.012386265210807323, -0.007345925085246563, -0.0059326994232833385, 0.0012232467997819185, 0.02737661637365818, 0.0038059207145124674, 0.013677303679287434, 0.002089955611154437, 0.0041382089257240295, 0.008175721392035484, 0.09378590434789658, -0.001165886176750064, 0.01438823714852333, 0.02309589646756649, 0.000680590863339603, -0.013379745185375214, -0.00014001726231072098, -0.01901184767484665, -0.0037063329946249723, 0.010051061399281025, -0.012579120695590973, -0.009601125493645668, 0.012617782689630985, 0.012859842739999294, 0.004886328708380461, 0.009092062711715698, -0.0030681847129017115, -0.009541511535644531, 0.01793203316628933, -0.002616175217553973, -0.0030907485634088516, 0.007287723012268543, -0.008655978366732597, -0.01753891445696354, 0.01075720600783825, -0.0280048456043005, 0.008579451590776443, -0.021763160824775696, -0.005743036977946758, 0.009215370751917362, -0.004947655834257603, -0.006942472420632839, 0.006494154222309589, 0.010513263754546642, 0.003461346961557865, 0.013463298790156841, -0.010295895859599113, 0.009883003309369087, -0.002619424369186163, -0.010814681649208069, 0.001052380190230906, -0.005923808552324772, -0.003298367140814662, 0.012645446695387363, -0.0011562822619453073, 0.005589002277702093, -0.018622247502207756, -0.004405265208333731, -0.014452275820076466, -0.0026866584084928036, -0.0013150088489055634, -0.00401536887511611, 0.011327248066663742, 0.008686566725373268, 0.007146268151700497, 0.0001866282254923135, 0.004476736765354872, -0.009989669546484947, -0.009388312697410583, -0.01893782988190651, 0.0009559312602505088, 0.010790102183818817, -0.004740056116133928, -0.00910812709480524, -0.0024608662351965904, -0.012401445768773556, -0.0018534656846895814, -0.0054603079333901405, -0.01949860341846943, 0.0041955625638365746, -0.008055292069911957, -0.009824808686971664, -0.006319538690149784, 0.015026931650936604, 0.041833795607089996, 0.014727109111845493, -0.005369856953620911, -0.0104244789108634, -0.0038949435111135244, -0.003833286464214325, -0.001637271954677999, 0.005627532955259085, 0.005987919867038727, -0.019941681995987892, 0.009401549585163593, -0.016592569649219513, -0.00481637567281723, -0.005577788222581148, -0.0015299554215744138, -0.006335475482046604, -0.009916997514665127, 0.0045005204156041145, -0.006080114282667637, -0.007462998852133751, 0.006761027500033379, 0.0007970434962771833, 0.08332464098930359, 0.004015937447547913, -0.019152158871293068, 0.008129376918077469, 0.009040029719471931, 0.01077691838145256, 0.005819081328809261, 0.0022804834879934788, 0.026350269094109535, 0.003214799566194415, 0.01737646386027336, 0.00012170330592198297, 0.009235079400241375, -0.00175623525865376, -0.001689879922196269, 0.002481182338669896, -0.009103959426283836, 0.010298890061676502, 0.006491833832114935, -0.015552589669823647, -0.0027703174855560064, 0.009428600780665874, -0.007206527981907129, 0.004612456075847149, -0.015710564330220222, 0.0024211248382925987, 0.0002556598628871143, -0.012880470603704453, -0.005563025362789631, -0.006357715930789709, -0.008722049184143543, -0.012452876195311546, 0.0026872067246586084, 0.011273089796304703, -0.00222362345084548, 0.0019123679958283901, 0.002393947681412101, 0.009942241944372654, 0.005812611896544695, -0.006896663457155228, 0.008762495592236519, 0.015075936913490295, -0.0018306146375834942, 0.0060026333667337894, -0.018397295847535133, -0.0038662953302264214, -0.006326139438897371, 0.014784881845116615, -0.002163069089874625, 0.001801275764591992, -0.007970981299877167, -0.009136003442108631, -0.01618318259716034, -0.0028799790889024734, -0.003107442520558834, -0.0022691157646477222, -0.0019820043817162514, -0.0030494481325149536, -0.007886171340942383, 0.025890560820698738, -0.0010657594539225101, -0.016829516738653183, 0.007582900580018759, 0.005752018187195063, -0.021554574370384216, 0.0036691557615995407, 0.013257135637104511, 0.011776287108659744, 0.0014969018520787358, 0.0054893819615244865, -0.0012511516688391566, 0.018096478655934334, -0.004348302725702524, 0.021253829821944237, 0.002041077008470893, 0.014675034210085869, -0.011642410419881344, -0.009472078643739223, -0.007605202961713076, 0.008670571260154247, 0.003685439471155405, -0.003906056983396411, -0.008088418282568455, 0.008212129585444927, 0.00983371865004301, 0.013162923976778984, 0.004394227638840675, 0.015969401225447655, 0.02249715104699135, -0.006875868421047926, -0.0018449377967044711, -0.016396289691329002, -0.00920373760163784, 0.007722979877144098, -0.010056684724986553, -0.0012909520883113146, -0.009844930842518806, 0.001633345615118742, -0.011350569315254688, 0.010802256874740124, -0.005172785371541977, 0.010712745599448681, -0.009635989554226398, -0.015681259334087372, 0.010590653866529465, -0.0061666760593652725, -0.005514507181942463, -0.004461857955902815, 0.003558896016329527, 0.004388376139104366, -0.029346365481615067, 0.00020702053734567016, -0.007121792063117027, 0.0005941647686995566, 0.00804111734032631, -0.021338697522878647, -0.0025723772123456, -0.009236986748874187, -0.0008349426789209247, -0.00982227735221386, 5.9105541367898695e-06, -0.011921286582946777, -0.00894884578883648, 0.014792678877711296, -0.006550110876560211, -0.014504891820251942, 0.0024900506250560284, 0.005497332196682692, 0.0009826344903558493, -0.0011663512559607625, 0.017316648736596107, -0.0022777514532208443, -0.005813748110085726, -0.010394947603344917, 0.01879136450588703, 0.003986380062997341, -0.011821753345429897, 0.0191112719476223, -0.013518418185412884, 0.006668406073004007, 0.008293918333947659, -0.015851307660341263, -0.019959595054388046, -0.007447301875799894, 0.011714155785739422, 0.007041063159704208, -0.014657826162874699, -0.006393141113221645, 0.00039681282942183316, -0.014382807537913322, -0.021124834194779396, 0.005088697653263807, -0.006019861903041601, 0.0052501908503472805, 0.005430873017758131, -0.006157414522022009, 0.0015928137581795454, 0.003679470391944051, -0.008318174630403519, -0.011522086337208748, 0.02121584862470627, -0.06031360477209091, 0.006917450577020645, 0.0025965797249227762, 0.004059139173477888, -0.003948066849261522, 0.0004576897481456399, -0.00523928040638566, -0.0005621068412438035, 0.010063822381198406, -0.017914773896336555, -0.005314290057867765, -0.01484303455799818, 0.00022231806360650808, 0.010761556215584278, -0.017128387466073036, 0.012173475697636604, -0.0011957817478105426, 0.012898216024041176, -0.005369173362851143, -0.004420273005962372, -0.0006196373142302036, 0.0025067406240850687, -0.01168831530958414, 0.01117966789752245, 0.022111406549811363, 0.0006548913079313934, -0.0006875720573589206, 0.005936674773693085, -6.805764132877812e-05, 7.900774653535336e-05, -0.0014314241707324982, -0.011147154495120049, 0.006290697492659092, 0.020954493433237076, 0.01088282372802496, -0.0007316836854442954, 0.0030415868386626244, -0.0012748305452987552, 0.0036636514123529196, -0.010949750430881977, -0.011532105505466461, -0.0049323104321956635, -0.00018733477918431163, 0.005096959415823221, -0.014694340527057648, 0.0028503506910055876, 0.022797031328082085, -0.0030924901366233826, 0.02174854278564453, -0.003067219629883766, 0.006617911159992218, 0.019078684970736504, -0.005146028008311987, 0.007592090871185064, -0.0029291589744389057, -2.1114541596034542e-05, -0.00101366825401783, 0.0015199838671833277, 0.002870461205020547, -0.00709689361974597, 0.01488563697785139, -0.0001753609103616327, -0.006439183838665485, 0.007206756621599197, -0.010649329982697964, 0.006107760593295097, 0.012424701824784279, 0.010081454180181026, -0.022923221811652184, -0.009457983076572418, 0.006372913718223572, 0.00967329926788807, -0.015781553462147713, -0.0065523786470294, -0.006723855622112751, 0.0016048181569203734, 0.00426945835351944, 0.019162926822900772, 0.002784224459901452, 0.010740403085947037, 0.007713716011494398, 0.011921608820557594, 0.013596885837614536, -0.015241366811096668, 0.01747843623161316, -0.0020391319412738085, 0.00025371884112246335, 0.0002239910390926525, -0.021796688437461853, -0.010243941098451614, 0.002743021585047245, 0.003116446314379573, 0.0008654974517412484, 0.004982560873031616, -0.0017711423570290208, 0.018063634634017944, 0.01420375145971775, 0.00542053859680891, 0.013404542580246925, 0.0037796124815940857, -0.0184597447514534, -0.03281616047024727, 0.0041008200496435165, 0.013596110977232456, -0.010426509194076061, 0.009346285834908485, -0.014257697388529778, 0.0015296210767701268, 0.002986632287502289, -0.00019413113477639854, 0.004568730015307665, -0.002179908100515604, 0.0009267341229133308, -0.0074128094129264355, -0.015964452177286148, -0.0011179184075444937, 0.0022412321995943785, -0.004298115149140358, -0.0004162184486631304, 0.0014280450996011496, -0.009912355802953243, 0.006910561118274927, 0.005097000859677792, -0.01800219714641571, 0.01439499482512474, -0.0061784773133695126, -0.00819759164005518, 0.008580327033996582, 0.0044676573015749454, -0.008678324520587921, 0.0006821280694566667, 0.0061704558320343494, -0.0008810217841528356, 0.002721853321418166, 0.002718100557103753, 0.007166680414229631, 0.011545474641025066, 0.023633914068341255, -0.007805683184415102, -0.01132554467767477, 0.00015082604659255594, 0.0031502917408943176, 0.012858198955655098, -0.002457505324855447, 0.01268454547971487, -0.005959332454949617, 0.01930583268404007, 0.000984208774752915, 0.018037816509604454, 0.006561052054166794, -0.005797810852527618, 0.014617260545492172, 0.02134053036570549, 0.003991372417658567, -0.002691415371373296, -0.01268844772130251, 0.009914965368807316, -0.004740691743791103, -0.000983863021247089, -0.0009204798843711615, 0.007801236119121313, -0.00336856534704566, 0.008007959462702274, -0.003081487724557519, 0.015544066205620766, -0.012083322741091251, -0.002105755964294076, 0.0005764501402154565, -0.007332124747335911, -0.011123448610305786, 0.010771570727229118, -0.012784790247678757, -0.01544584333896637, 0.01441658940166235, -0.007293094415217638, -0.015383492223918438, 0.005893138237297535, -0.0063249594531953335, -0.003544497536495328, 0.006646256428211927, -0.017420588061213493, 0.0010037708561867476, -0.0018982220208272338, 0.0034584093373268843, 0.006218425929546356, -0.001025506528094411, 0.002053579781204462, 0.0005409293808043003, 0.004587043076753616, 0.014824953861534595, 0.007324757520109415, 0.00466138357296586, -0.029703481122851372, -0.004515799693763256, 0.0006597661413252354, 0.004691724665462971, -0.00820132065564394, -0.0059221419505774975, 0.018144875764846802, -0.00037093376158736646, -0.010413948446512222, -0.013153934851288795, 0.019855637103319168, -0.013586221262812614, 0.012597751803696156, -0.1076473519206047, -0.00723522063344717, 0.0007123809773474932, -0.0020223099272698164, -0.007248611189424992, -0.004780794959515333, 0.019857129082083702, -0.017489757388830185, -0.008182063698768616, -0.015798207372426987, 0.008198562078177929, -0.013349568471312523, -0.011583741754293442, -0.033705778419971466, -0.0028869525995105505, 0.007089974358677864, 0.004725117702037096, -0.023175431415438652, 0.004312857985496521, 0.015592322684824467, -0.006876148749142885, -0.0025501293130218983, -0.013571109622716904, 0.00326846307143569, -0.007737492676824331, -0.004958984442055225, -0.01244539488106966, -0.002879844978451729, -0.004989580251276493, 0.028435971587896347, -0.008701689541339874, 0.004599238745868206, 0.006427035667002201, -0.004361109808087349, 0.013628292828798294, -0.007047416176646948, 0.008524023927748203, 0.007645521312952042, -0.13263514637947083, -0.004020132124423981, 0.003942784853279591, -0.016674863174557686, -0.0012385814916342497, 0.015231072902679443, -0.005411607213318348, -0.007878675125539303, 0.004778949078172445, -0.001167198526673019, -0.0042077722027897835, -0.011381981894373894, -0.005144424736499786, -0.008606525138020515, 0.005931728985160589, -0.008526775985956192, -0.025564882904291153, 0.023580553010106087, -0.0011002783430740237, -0.007845158688724041, 0.005135186482220888, 0.0029120808467268944, 0.009909023530781269, 0.007341313175857067, 0.0036838268861174583, -0.012222465127706528, 0.008004467934370041, 0.009153622202575207, 0.016203289851546288, -0.004891273565590382, 0.022191010415554047, 0.006580699700862169, -0.0051149711944162846, 0.008745936676859856, 0.021879611536860466, 0.021640628576278687, -0.0029249759390950203, 0.011976630426943302, 0.0022016216535121202, -0.002100847428664565, 0.012602762319147587, 0.02311176061630249, 0.008927162736654282, 0.014750543981790543, 0.011661405675113201, -0.0040100133046507835, 0.002547171898186207, 0.008176642470061779, 0.0029579007532447577, -0.006715219933539629, 0.0091573940590024, 0.001137498882599175, -0.005689001642167568, 0.006293172482401133, 0.001004791702143848, 0.004942979197949171, 1.2983192391402554e-05, 0.002776492852717638, 0.01155821792781353, 0.002307955641299486, -0.006546309683471918, -0.016894035041332245, -0.0014984633307904005, 0.004226695746183395, 0.0017887544818222523, -0.0033998817671090364, 0.01627836562693119, 0.0056996396742761135, -0.008619751781225204, 0.0017954835202544928, 0.005020207725465298, 0.02093588560819626, 0.008480323478579521, -0.016279729083180428, -0.023909810930490494, -0.01178758218884468, 0.017065394669771194, 0.010937071405351162, -0.01882997527718544, -0.00023371088900603354, 0.01539680827409029, 0.002190994331613183, 0.00182085030246526, -0.0012696634512394667, -0.0014234873233363032, -0.006662747822701931, 0.0063637723214924335, -0.004932392854243517, 0.00364022352732718, -0.0392729789018631, -0.001252252608537674, 0.008313602767884731, -0.012586381286382675, 0.00322997965849936, -0.019817477092146873, -0.008517958223819733, -0.02155313454568386, -0.00019366366905160248, 0.004834326449781656, 0.0005430726450867951, 0.033606529235839844, -0.00029803323559463024, 0.009309569373726845, 0.004758809227496386, 0.003962131682783365, -0.006122655235230923, -0.01455635391175747, -0.012924205511808395, 0.005281707737594843, -0.012622407637536526, -0.006677051540464163, -0.022990375757217407, 0.017360277473926544, 0.017802687361836433, -0.023882517591118813, 0.01669805310666561, -0.025536030530929565, -0.0040024262852966785, -0.016901105642318726, -0.0032039338257163763, -0.016797108575701714, 0.014113388024270535, 0.011638456955552101, -0.011014807969331741, 0.02577820047736168, 0.00316159357316792, 0.02415228635072708, -0.02655906043946743, -0.0064916107803583145, 0.010503973811864853, 0.003478024620562792, -0.008697272278368473, -0.009536058641970158, 0.010516803711652756, 5.3849921641813125e-06, 0.011721871793270111, 0.002238920656964183, 0.019649064168334007, -0.012855407781898975, -0.014396333135664463, -0.009122959338128567, 0.009956290014088154, -0.017112312838435173, 0.006591572891920805, -0.0012147531379014254, 0.015569156967103481, 0.006980262231081724, 0.00018629759142640978, -0.004953069146722555, -0.0029862697701901197, -0.018230075016617775, -0.004150106571614742, -0.010113196447491646, 0.003924478776752949, -0.004456379916518927, -0.0016025586519390345, -0.026390070095658302, -0.011597920209169388, 0.006403652019798756, -0.00958871841430664, -0.006394336465746164, 0.0013902768259868026, -0.012931604869663715, -0.004174347966909409, 0.0004648423637263477, 0.024170953780412674, 0.002886270172894001, -0.030529895797371864, 0.015311618335545063, -0.003828235901892185, -0.006515458691865206, 0.0017830729484558105, -0.008579840883612633, 0.013079244643449783, 0.0029220646247267723, -0.009432089515030384, -0.00812060758471489, -0.0022381881717592478, 0.026811107993125916, 0.01964288204908371, -0.012076032347977161, -0.011309574358165264, -0.004275328014045954, -0.015420744195580482, 0.03280337154865265, -0.004843683447688818, -0.011989710852503777, -0.008978010155260563, -0.004080540034919977, -0.01492864266037941, -0.008923674933612347, -0.0082711698487401, 0.010436603799462318, -0.015860561281442642, -0.002495874185115099, 0.009511117823421955, 0.0011404057731851935, -0.0033643031492829323, 0.0059378682635724545, 0.013501917012035847, 0.008448618464171886, 0.0006864697788842022, 0.01042186189442873, -0.1348380148410797, 0.004177234135568142, -0.0015929262153804302, -0.01939922384917736, -0.006744918413460255, -0.014928440563380718, -0.007023122161626816, 0.00041094468906521797, 0.018726227805018425, -0.027765842154622078, 0.003943184390664101, -8.491543849231675e-05, -0.00018129248928744346, -0.012785973958671093, 0.007022008299827576, -0.015521280467510223, 0.00936637632548809, 0.020988527685403824, -0.013797763735055923, -0.004144972190260887, -0.026158176362514496, -0.01118401251733303, -0.02206113003194332, 0.005199471488595009, -0.037203073501586914, -0.007156831678003073, -0.0143052376806736, -0.003514890791848302, -0.0031357875559479, 0.0035249919164925814, 0.008044340647757053, -0.004336517304182053, -0.004324423614889383, 0.005740475840866566, 0.0013804258778691292, -0.00544745521619916, 0.011916503310203552, 0.004500718787312508, -0.009991337545216084, 0.04022327810525894, -0.005114453844726086, 0.004551529418677092, -0.02060248889029026, 0.003119345288723707, 0.007235264405608177, 0.0030220814514905214, -0.02900771051645279, -0.023122379556298256, -0.014417995698750019, -0.005312000866979361, -0.014877868816256523, -0.0028498934116214514, 0.005631375592201948, -0.0050279232673347, 0.010108738206326962, -0.028686515986919403, 0.0016856958391144872, 0.007702157367020845, -0.01280878484249115, -0.007656225468963385, 0.015806008130311966, -0.0036221304908394814, 0.0007337133283726871, 0.011124907061457634, 0.023060530424118042, -0.016133103519678116, 0.00935949757695198, 0.16485388576984406, -0.0023343064822256565, -0.0016358165303245187, -0.007942445576190948, -0.014351037330925465, -0.0044353376142680645, 0.01421650405973196, -0.006627182010561228, -0.003420105902478099, -0.01937832497060299, 0.024734344333410263, 0.016018345952033997, -0.011150367558002472, -0.008652250282466412, -0.006553378887474537, -0.004461463075131178, 0.0064875343814492226, -0.009933436289429665, 0.002258383436128497, -0.0013844402274116874, -0.013114762492477894, -0.013891172595322132, 0.0019321291474625468, -0.02652677148580551, 0.016079582273960114, -0.010148628614842892, 0.005361219402402639, 0.02914300002157688, 0.00991654023528099, 0.003569973399862647, 0.0076081156730651855, -0.024209383875131607, 0.000911596929654479, -0.01925239898264408, -0.0017878457438200712, 0.015870383009314537, -0.012609270401299, 0.009261883795261383, 0.00728247407823801, -0.00376332551240921, 0.022861666977405548, -0.00919640064239502, -0.015062268823385239, -0.008080716244876385, -0.006774331443011761, -0.022667013108730316, -0.005625760182738304, -0.004152765031903982, 0.0011411431478336453, -0.004528135526925325, -0.013897971250116825, -0.009348820894956589, -0.007455325685441494, 0.014162336476147175, 0.011412370018661022, 0.013633697293698788, -0.022202065214514732, -0.00045988752390258014, 0.0028680693358182907, -0.00944735947996378, -0.007330194115638733, 6.317395309451967e-05, -0.02317420393228531, -0.014790121465921402, 0.0024852147325873375, 0.017514057457447052, -0.004182170610874891, -0.03406967222690582, 0.011228714138269424, -0.1244254931807518, 0.006296620704233646, -0.019423309713602066, -0.020825156942009926, -0.009901251643896103, 0.0021920581348240376, 0.024839699268341064, 0.03518237546086311, 0.019685300067067146, 0.012196478433907032, 0.007373434957116842, 0.009603729471564293, -0.014683319255709648, 0.016667239367961884, 0.010603442788124084, 0.016505567356944084, 0.0009999608155339956, -0.0015780298272147775, 0.03467750921845436, 0.010037226602435112, -0.010064533911645412, 0.01960126869380474, 0.004729357548058033, -0.002154436195269227, 0.008662763051688671, 0.013781355693936348, -0.01684243604540825, 0.006379368714988232, -0.006057994905859232, -0.022148653864860535, 0.008242874406278133, 0.030508698895573616, -0.010978372767567635, 0.004483467899262905, -0.0036923105362802744, -0.00838402658700943, 0.015929408371448517, 0.019319556653499603, -0.0012297899229452014, 0.008937019854784012, -0.00012469696230255067, 0.006659781094640493, -0.012048798613250256, 0.0002646470384206623, -0.003412610152736306, 0.012642069719731808, 0.013798867352306843, -0.021662583574652672, -0.0021118326112627983, -0.006650259718298912, 0.002875769278034568, -0.01656060293316841, -0.01298945676535368, 0.005857702810317278, 0.010867110453546047, -0.009051748551428318, 0.02947556972503662, -0.021449483931064606, 0.004059482365846634, 0.002315693302080035, -0.014033970423042774, 0.020138829946517944, 0.018799955025315285, -0.005676714237779379, 0.0062441457994282246, -0.021657494828104973, -0.0020670986268669367, -0.003643258009105921, 0.024800308048725128, 0.001759265549480915, -0.0281737819314003, -0.009120051749050617, 0.011548852548003197, 0.0018545633647590876, 0.00602008868008852, -0.02595554105937481, 0.011022496968507767, 0.0011507488088682294, -0.0021056176628917456, 0.010616408661007881, 0.005103547591716051, 0.0044352333061397076, 0.002033558674156666, -0.014532445929944515, 0.05388914793729782, -0.008297284133732319, -0.009412514045834541, -0.012229613028466702, 0.012323681265115738, 0.018291091546416283, -0.010210246779024601, 0.023636123165488243, 0.0072483886033296585, 0.001840917975641787, 0.015493523329496384, 0.021333646029233932, -0.004910225979983807, 0.02131631225347519, 0.003290375228971243, -0.020018363371491432, 0.0049929553642869, -0.00676412507891655, 0.0038143452256917953, 0.02401747927069664, 0.01440659910440445, -0.002988494699820876, 0.007410991936922073, 0.003441218053922057, 0.002943882253021002, 0.0135292187333107, 0.012910944409668446, -0.014082680456340313, 0.005304886028170586, 0.007101838942617178, 0.016762277111411095, 0.03087615966796875, -0.004992553498595953, -0.005020202603191137, 0.0014249024679884315, -0.010307428427040577, 0.017609313130378723, -0.0022981977090239525, 0.00315157906152308, -0.0041457111947238445, -0.006058716680854559, 0.00894047599285841, 0.008043394424021244, 0.007152414880692959, 0.03743710741400719, 0.020348189398646355, 0.004007161594927311, 0.02948322333395481, -0.004564314614981413, -0.009192478843033314, 0.012805177830159664, -0.005346042104065418, 0.03234764561057091, -0.007531913463026285, -0.01700618490576744, -0.004680939018726349, 0.013695272617042065, -0.014647185802459717, -0.0019884025678038597, 0.0304054357111454, -0.01739390939474106, -0.000914381816983223, 0.013306712731719017, 0.011422563344240189, 0.005350591614842415, -0.0077351792715489864, -0.03123105689883232, -0.012427769601345062, -0.020723305642604828, 0.005386835429817438, 0.006032547913491726, 0.007149749901145697, 0.00842262152582407, -0.003654671832919121, -0.011768453754484653, -0.012387492693960667, -0.02708388864994049, -0.0011771139688789845, -0.004974993411451578, 0.00810147263109684, 0.012108616530895233, 0.003622549818828702, -0.0017986512975767255, -0.019709529355168343, -0.016830332577228546, 0.00031068208045326173, 0.029062410816550255, 0.007662045303732157, -0.0032731918618083, 0.000989978201687336, -7.383248157566413e-05, 0.02173413150012493, 0.018253210932016373, -0.008708122186362743, 0.003768555587157607, -0.07770949602127075, 0.007290242705494165, 0.011157583445310593, -0.013044144958257675, 0.0016936102183535695, -0.008832489140331745, 0.0019144826801493764, -0.012308274395763874, -0.03703510761260986, -0.02569892443716526, 0.026658715680241585, -0.018253471702337265, 0.0011529993498697877, -0.0035613325890153646, -0.021146515384316444, 0.0323844775557518, -0.017551133409142494, -0.011165432631969452, 0.007089164573699236, 0.01229042001068592, 0.007409955840557814, 0.005163088906556368, 0.005250937771052122, 0.008845852687954903, -0.012731275521218777, -0.015035467222332954, -0.007809184491634369, 0.019557608291506767, -0.010132893919944763, -0.0060460916720330715, 0.014628286473453045, -0.002818026579916477, -0.014853931032121181, -0.03644948825240135, -0.016655592247843742, 0.00030313487513922155, -0.011562255211174488, -0.01687578111886978, 0.008698445744812489, -0.032079067081213, 0.01996234431862831, 0.005404299590736628, -0.09511695057153702, -0.0007755542756058276, -0.000680362107232213, -0.00014651569654233754, 0.022435113787651062, -0.013644799590110779, 0.004703387152403593, -0.013985254801809788, 0.007137341424822807, 0.0002729800471570343, 0.004633347038179636, 0.004091928247362375, -0.004793011583387852, 0.006192969158291817, -0.012287701480090618, 0.009760797023773193, -0.0024225274100899696, -0.013304273597896099, -0.025770146399736404, -0.022104907780885696, 0.01872146688401699, -0.01396960113197565, 0.002309646224603057, 0.00683793518692255, -0.014403645880520344, 0.009703029878437519, -0.004891805816441774, 0.03337075188755989, 0.004819423891603947, -0.013712783344089985, -0.005514001008123159, 0.0042887660674750805, 0.024308549240231514, 0.010968245565891266, 0.01088013220578432, 0.003959304187446833, 0.004231554456055164, 0.019619300961494446, 0.026182960718870163, 0.02286560833454132, 0.00955805554986, 0.011938193812966347, 0.00667917262762785, 0.00037227338179945946, 0.01669689640402794, -0.1197676733136177, -0.0002679718309082091, 0.0011811986332759261, -0.019013244658708572, 0.019541118294000626, 0.005745868198573589, 0.0041518667712807655, 0.10716651380062103, 0.002193667460232973, -0.03058452159166336, -0.0026946449652314186, 0.007021752651780844, -0.019001128152012825, -0.006546063348650932, -0.009224442765116692, 0.002154486021026969, 0.012490476481616497, 0.011120056733489037, 0.0015903855673968792, -0.007961893454194069, -0.027321703732013702, -0.005017496179789305, 0.011072317138314247, -0.02522849105298519, 0.010922970250248909, -0.06516678631305695, -0.012962841428816319, 0.004688472021371126, -0.005950085818767548, 0.011720702983438969, -0.011569565162062645, -0.024725351482629776, -0.013118403032422066, -0.0017148819752037525, -0.0005957928369753063, 0.014579419046640396, -0.001382474321871996, 0.003967319615185261, -0.0012498062569648027, 0.01405593566596508, -0.017815636470913887, 0.005363635253161192, 0.016577288508415222, -0.020331867039203644, 0.010641631670296192, -0.003335685236379504, -0.007181367836892605, 0.02538468688726425, -0.03842005878686905, 0.00042200525058433414, 0.0018073560204356909, -0.024537468329072, -0.004673693794757128, -0.008599611930549145, 0.014544245786964893, -0.009674504399299622, -0.012573564425110817, -0.013823259621858597, -0.0027538216672837734, 0.006078389007598162, -0.005533628631383181, -0.0010111534502357244, -0.005318010691553354, -0.0038460774812847376, -0.010283023118972778, 0.010618564672768116, -0.01743711344897747, -0.020936686545610428, -0.015189951285719872, 0.007061748765408993, 0.015320578590035439, 0.01379713136702776, 0.027479203417897224, -0.006004102993756533, 0.008280807174742222, -0.01872536912560463, -0.015889715403318405, 0.005293151363730431, -0.01976575143635273, -0.02178427204489708, 0.01082679908722639, 0.014926794916391373, 0.01651175506412983, -0.014258751645684242, -0.0008192502427846193, -0.001580232521519065, 0.006466224789619446, 0.0173360463231802, 0.005358257330954075, 0.024113915860652924, -0.001138958497904241, 0.017149604856967926, 0.004240833222866058, -0.012876348569989204, -0.0022212453186511993, 0.014303356409072876, -0.003691335441544652, 0.002679812256246805, 0.0016480851918458939, 0.0005948570906184614, 0.007658522110432386, 0.03484295308589935, 0.00017304570064879954, -0.017466573044657707, -0.013646271079778671, 0.005076609551906586, 0.0008128646295517683, 0.010173222050070763, -0.0038050932344049215, 0.0033299820497632027, 0.013884719461202621, -0.01435771118849516, -0.013851840049028397, 0.006598439533263445, 0.03276501223444939, -0.012565518729388714, -0.00922292098402977, -0.004122341983020306, -0.027973024174571037, -0.0002571498625911772, 0.007557202596217394, 0.002681559417396784, -0.011944305151700974, -0.015582959167659283, 0.008422142826020718, -0.0049859946593642235, -0.005566791165620089, -0.0272283386439085, -0.0033743262756615877, 0.00373673508875072, 0.028188349679112434, 0.008979322388768196, -0.020740164443850517, -0.0028888657689094543, 0.0009580907062627375, 0.0021482042502611876, 0.004002775996923447, 0.015370077453553677, -0.023697618395090103, -0.0038452749140560627, -0.0001904294331325218, -0.01445489376783371, -0.003475708421319723, 0.005496329162269831, 0.013971527107059956, 0.008112091571092606, -0.0008314648875966668, 0.004665221553295851, -0.005783579312264919, -0.0014098092215135694, -0.017778584733605385, 0.025294553488492966, 0.0056548104621469975, -0.014151240698993206, 0.012171437032520771, 0.002688990207388997, 0.01643485762178898, 0.005331507883965969, 0.01787497103214264, -0.012368883937597275, 0.00867017824202776, -0.004896694328635931, 0.0028753287624567747, 0.027523944154381752, -0.008098820224404335, -0.0010554061736911535, 0.008267812430858612, -0.009985150769352913, 0.002499455353245139, -0.0031236843205988407, 0.04178006574511528, -0.01642834022641182, 0.010983899235725403, 0.00363841000944376, 0.03235780447721481, -0.011745870113372803, -0.0005992952501401305, 0.0003396125102881342, 0.0018654746236279607, -0.003409497905522585, 0.001001924742013216, 0.008349098265171051, -0.011317380703985691, -0.0029860823415219784, 0.009884475730359554, 0.01794380508363247, 0.004686318803578615, -0.01011921837925911, 0.022739751264452934, -0.019995801150798798, 0.010847951285541058, 0.016834255307912827, -0.0048689646646380424, -0.008150995709002018, 0.0050621647387743, 0.010394170880317688, -0.004421773832291365, -0.010828118771314621, 0.015071727335453033, 0.015654847025871277, 0.014634993858635426, -0.0034323239233344793, -0.010356404818594456, -0.01025665644556284, 0.003173207398504019, 0.005011940374970436, -0.034552477300167084, 0.02061515673995018, -0.006420977413654327, 0.005674912128597498, 0.001060189912095666, -0.00647811871021986, 0.0035320071037858725, 0.014523692429065704, -0.0010079023195430636, 0.012613195925951004, 0.022783488035202026, -0.02854624018073082, -0.0019005016656592488, 0.0048736026510596275, 0.01074291579425335, 0.0017360917991027236, 0.0009135931613855064, 0.01136306207627058, -0.012216472066938877, -0.00061573158018291, 0.005930843763053417, -0.007330005522817373, 0.0006138037424534559, 0.003615995403379202, 0.0028281027916818857, 0.0017319181933999062, -0.0005196588463149965, 4.629351678886451e-05, -0.025416307151317596, -0.037752602249383926, -0.01215266715735197, -0.023849232122302055, -1.4213692338671535e-05, -0.001388812204822898, -0.0015339609235525131, 0.0065259188413619995, 0.0006459835567511618, -4.296947372495197e-05, -0.010023152455687523, -0.007894477806985378, -0.007713627070188522, -0.009382683783769608, -0.004857551772147417, 0.006140642333775759, -0.00157629803288728, -0.007621290627866983, 0.005803065840154886, -0.022206222638487816, -0.0034239445813000202, 0.005160510540008545, -0.0037929038517177105, 0.0029246548656374216, -0.0035703573375940323, 0.01179152075201273, 0.03409230336546898, 0.0092979297041893, -0.00613282760605216, 0.00782342255115509, -0.02441238984465599, 0.00984582956880331, 0.0037543175276368856, 4.395019277581014e-05, -0.027660371735692024, 0.012625782750546932, 0.027683837339282036, -0.014833223074674606, -0.02996239997446537, 0.00969600211828947, -0.0063352035358548164, -0.0034649320878088474, 0.002271464327350259, 0.006218577269464731, 0.0005078351241536438, 0.018734967336058617, 0.00910938624292612, -0.0012264872202649713, 0.008284795098006725, 0.012430757284164429, 0.005730191245675087, -0.014890541322529316, 0.028736228123307228, -0.0018142798217013478, 0.0009475177503190935, -0.010430079884827137, 0.005570776294916868, 0.006227846257388592, 0.010723303072154522, -0.0030596796423196793, -0.025962000712752342, -0.039370208978652954, 0.013002255000174046, 0.0015269488794729114, -0.012038704939186573, 0.010218500159680843, -0.0020185422617942095, -0.010749232955276966, -0.013674859888851643, -0.017939068377017975, -0.004821692127734423, 0.0036286073736846447, 0.019266456365585327, 0.0035632646176964045, -0.010580945760011673, -0.0015183323994278908, 0.0037549762055277824, -0.009843505918979645, 0.019854579120874405, 0.01384916715323925, -0.02867267280817032, -0.004765793215483427, 0.010295775718986988, 0.0006748687592335045, 0.002666880376636982, -0.0027768113650381565, -0.01511292066425085, 0.006755774840712547, 0.016504263505339622, 0.01078652124851942, 0.008027956821024418, 0.012385114096105099, 0.030641155317425728, -0.0005567778134718537, -0.0008513397187925875, -0.02524569071829319, -0.02416769042611122, 0.007490529213100672, -0.005536300130188465, -0.0018703036475926638, 0.017046557739377022, 0.011900006793439388, -0.010101817548274994, 0.018548108637332916, 0.012109329923987389, 0.01966463029384613, -0.015239929780364037, 0.012718643993139267, 0.007269838824868202, -0.02073613740503788, 0.019215155392885208, 1.8954326151288114e-05, -0.010207444429397583, -0.0015237407060340047, 0.008070421405136585, -0.009663160890340805, -0.006891086231917143, 0.007607459556311369, 0.005954877473413944, 0.008658559061586857, 0.010052413679659367, -0.0026660903822630644, -0.011954267509281635, -0.01142663974314928, 0.02003946155309677, 0.006150881759822369, 0.007936927489936352, -0.006752899847924709, 0.00544177507981658, 0.011908323504030704, 0.016225697472691536, -0.0050387014634907246, 0.0021109168883413076, -0.03169659525156021, -0.01619897224009037, -0.0007703203009441495, 8.629521471448243e-05, 0.0010042230132967234, -0.02213233709335327, -0.010758553631603718, -0.011119608767330647, -0.003622503485530615, -0.02492593787610531, 0.004821347072720528, -0.009320111945271492, -0.015448145568370819, -0.0005651611718349159, 0.00020887785649392754, -0.017979247495532036, -0.008656186051666737, 0.0014125594170764089, 0.010636170394718647, -0.004286757670342922, 0.010004770942032337, -0.006711128633469343, 0.009671610780060291, 0.0001517490454716608, 0.007308180443942547, 0.011582779698073864, 0.0001511629088781774, 0.005227291025221348, -0.00525309331715107, -0.013911325484514236, -0.012409470975399017, -0.01714172586798668, -0.0012289178557693958, -0.009810618124902248, -0.005948150064796209, 0.003891662461683154, 0.008757296949625015, 0.005751237738877535, -0.0011970873456448317, 0.020354503765702248, 0.015365717001259327, -0.0024607947561889887, 0.009550418704748154, 0.009769699536263943, -0.0015130401588976383, 0.016117842867970467, 0.005236098077148199, -0.015279747545719147, 0.013506961055099964, -0.009698474779725075, 0.005106382537633181, 0.0013198427623137832, 0.019235067069530487, -0.014965699054300785, -0.013156155124306679, -0.0005890098982490599, -0.0027701419312506914, -0.007449310272932053, -0.0011548828333616257, -0.0120107876136899, -0.019145814701914787, 0.030314190313220024, -0.006033594720065594, 0.013143743388354778, -0.007506218738853931, -0.0004665380984079093, 0.008268697187304497, -0.010413420386612415, 0.001087578828446567, -0.0194707028567791, 0.00945606455206871, -0.005049975123256445, 0.016847433522343636, 0.014914093539118767, 0.023083966225385666, -0.0011298551689833403, 0.004446530248969793, 0.0009230542927980423, 0.0073202792555093765, -0.0026034903712570667, 0.008866879157721996, -0.021769488230347633, 0.016724810004234314, -0.0053199948742985725, -0.01537367608398199, -0.0029164301231503487, 0.004684540443122387, 0.00037819560384377837, -0.0030824607238173485, 0.015129228122532368, -0.0056968810968101025, 0.012666088528931141, -0.018424993380904198, -0.012089535593986511, 0.003778196405619383, 0.004595635458827019, -0.008211148902773857, -0.0034001674503087997, -0.007960638962686062, -0.0009708101861178875, -0.009445550851523876, -0.01371467299759388, -0.014962807297706604, -0.030479300767183304, 0.0015499681467190385, -0.0567665733397007, 0.022877249866724014, 0.01202844362705946, -0.0016956203617155552, 0.026457522064447403, 0.0008449822198599577, 0.007149197161197662, -0.05228515341877937, 0.007666960824280977, 0.004749962594360113, -0.0033734783064574003, 0.018260320648550987, 0.0014495443319901824, -0.008396463468670845, 0.00152835703920573, 0.016482066363096237, -0.010889118537306786, -0.008941629901528358, -0.03341102600097656, 0.016827061772346497, -0.01867138221859932, 0.008941489271819592, 0.004619118757545948, -0.0060266489163041115, 0.00386616587638855, -0.01708097755908966, 0.005407622549682856, -0.01129680871963501, -0.005450356286019087, -0.012220142409205437, 0.021215315908193588, -0.012670828960835934, 0.0026220555882900953, 0.0008652001852169633, 0.02071787603199482, 0.018996909260749817, 0.0068143513053655624, -0.05469895899295807, -0.0046437084674835205, 0.0008842174429446459, 0.015333930030465126, 0.0002678986929822713, 0.015518136322498322, -0.017129380255937576, -0.00860968604683876, 0.018422305583953857, -0.013257474638521671, 0.0017671501263976097, -0.007982403971254826, 0.011437776498496532, -0.0007172739715315402, -0.032180506736040115, -0.00654223607853055, 0.0023724145721644163, -0.007891546003520489, -0.0001254249073099345, 0.013850986957550049, -0.00411599176004529, 0.003351099556311965, 0.0045462059788405895, -0.014054863713681698, 0.020840482786297798, -0.011523650959134102, 0.009297488257288933, -0.008747474290430546, -0.006789712235331535, -0.013833936303853989, -0.009562991559505463, 0.009847510606050491, -0.00872745644301176, -0.011077001690864563, 0.010407799854874611, 0.007620038464665413, -0.006661323364824057, 0.0069982754066586494, -0.016728604212403297, -0.01344231516122818, 0.0021744403056800365, -0.004003418143838644, -0.0023834938183426857, 0.02974877506494522, -0.016525156795978546, 0.0025834841653704643, -0.02001972496509552, 0.008305942639708519, -0.006860042456537485, 0.02060064673423767, 0.01068668719381094, -0.019591134041547775, -0.0005517256795428693, -0.0019336408004164696, -0.0016721595311537385, -0.01644769497215748, 0.004258826375007629, -0.0034443638287484646, -0.009049365296959877, 0.020297273993492126, 0.0402236171066761, -0.012901166453957558, -0.003748105140402913, 0.0008922318229451776, 0.017133694142103195, 0.022880462929606438, 0.035569362342357635, 0.006555237807333469, -0.004866652190685272, 0.0037520399782806635, 0.00018345967691857368, 0.0017672289395704865, 0.0014726795488968492, -0.00959673710167408, 0.006809554994106293, -0.007259634789079428, 0.0034339416306465864, 0.006659000646322966, -0.006825719960033894, 0.003601590869948268, 0.00634962972253561, -0.018997585400938988, -0.00505433976650238, 0.009987467899918556, -0.004694285802543163, 0.005398789886385202, -0.0067517259158194065, -0.012780616991221905, -0.007677262648940086, 0.009250618517398834, 0.010629433207213879, 0.0014435381162911654, -0.0034909099340438843, 0.0008536798413842916, 0.0003523837367538363, 0.00045579447760246694, 0.006832537241280079, 0.021629350259900093, -0.006299363914877176, -0.0029663729947060347, -0.024362847208976746, -0.018028540536761284, 0.0026179817505180836, -0.010541806928813457, 0.00750148156657815, 0.003360284259542823, -0.02661113440990448, 0.0043257297948002815, -0.026979563757777214, 0.01870860531926155, 0.008322197012603283, -0.013429082930088043, 0.002216180320829153, 0.001105220289900899, -0.0029584337025880814, -0.008152153342962265, 8.714382420293987e-05, 0.012141374871134758, 0.010948545299470425, -0.0032512398902326822, 0.003538332646712661, -0.009772373363375664, -0.006223761476576328, -0.009715733118355274, 0.015039591118693352, -0.02616751380264759, 0.02128325216472149, -0.004727799445390701, -0.002160412725061178, 0.0003339234972372651, 0.00539819523692131, 0.02376304194331169, -0.0021713750902563334, -0.00705938832834363, 0.006226509343832731, 0.008817285299301147, 0.006560558453202248, 0.025750242173671722, -0.014160947874188423, 0.013130019418895245, 0.01239712256938219, -0.008871188387274742, 0.003619680879637599, 0.010060188360512257, -0.014508301392197609, -0.007714997511357069, 0.022196093574166298, 0.017014378681778908, -0.007146788295358419, 0.018323566764593124, 0.006328250281512737, -0.009522109292447567, -0.004335344303399324, 0.0023124953731894493, -0.004555622581392527, 0.008589563891291618, 0.008943342603743076, 0.018259042873978615, 0.007268367800861597, -0.009770938195288181, -0.01613692380487919, -0.001054848893545568, -0.00025543422088958323, -0.015365125611424446, -0.001453657285310328, 0.003451024182140827, 0.006898172199726105, -0.003542232094332576, 0.02377176471054554, 0.008325117640197277, -0.0027308003045618534, -0.02452232502400875, 0.013318328186869621, 0.006352782249450684, 0.011135557666420937, 0.004601226653903723, -0.006763390731066465, 0.17747637629508972, 0.10216069966554642, -0.013987695798277855, -0.011849995702505112, 0.011831648647785187, -0.008159485645592213, 0.003571164794266224, 0.016672804951667786, 0.01987130381166935, 0.00016688648611307144, -0.01006250735372305, -0.012720275670289993, -0.010160029865801334, 0.005148051306605339, 0.016432590782642365, -0.01889961212873459, -0.004381826147437096, 0.005272671580314636, 0.0013939790660515428, 0.009678510017693043, -0.014982200227677822, 0.01036656554788351, -0.00786574650555849, -0.0087351119145751, -0.014524318277835846, -0.006528636906296015, -0.019901972264051437, 0.0023987703025341034, -0.0014309296384453773, 0.019250478595495224, -0.014855309389531612, -0.0020434220787137747, -0.013098962604999542, -0.005579764489084482, 0.010567763820290565, -0.00420769490301609, -0.01374741829931736, -0.019841644912958145, 0.0057880268432199955, 0.003019717987626791, -0.022371266037225723, 0.006137416232377291, -0.009741673246026039, -0.012050365097820759, 0.0006322301342152059, -0.00864062923938036, -0.015580152161419392, 0.003628554055467248, 0.0005287619424052536, -0.007670645136386156, -0.023414913564920425, 0.006192071363329887, 0.002509911311790347, 0.01650385744869709, 0.0020846265833824873, -0.0036579726729542017, 0.022575104609131813, -0.0021296534687280655, -0.03097594529390335, 0.007616014685481787, 0.006731057073920965, -0.015381255187094212, 0.012173417955636978, -0.004120634403079748, 0.009023926220834255, -0.007224990054965019, 0.004411148838698864, 0.0014800068456679583, -0.007584966719150543, 0.006377695593982935, -0.004077638033777475, 0.023020314052700996, -0.015979306772351265, 0.006413148250430822, -0.019000709056854248, 0.001823129365220666, 0.005231979303061962, 0.0007559424266219139, 0.0009134528809227049, -0.006613965146243572, -0.005489641800522804, -0.005901381839066744, -0.015039848163723946, 0.023785952478647232, 0.0021403534337878227, 0.010665844194591045, 0.008084678091108799, 0.020645704120397568, 0.11449828743934631, 0.011167431250214577, 0.002554314909502864, -0.009922643192112446, 0.009776790626347065, 0.0093989921733737, -0.011855141259729862, 0.04651356115937233, 0.00393309723585844, -0.006411741022020578, -0.004919785540550947, 0.011394538916647434, 0.006572223734110594, 0.0019573417957872152, 0.011369576677680016, 0.013790563680231571, 0.01882580853998661, 0.050724729895591736, 0.012148457579314709, -0.0014636908890679479, -0.012477327138185501, 0.006086581852287054, -0.009008824825286865, -0.010536180809140205, 0.006831394508481026, -0.007330639753490686, 0.008830582723021507, 0.01610601134598255, -0.009078588336706161, 0.01841220073401928, -0.0875713899731636, 0.0003436806728132069, -0.019023647531867027, -0.015278913080692291, -0.0002725996309891343, 0.0008294932777062058, -0.003795467782765627, -0.00771499564871192, -0.03500979021191597, 0.010278400965034962, -0.010892053134739399, -0.014119807630777359, -0.0009082432952709496, -0.005637738388031721, -0.007800987455993891, 0.026481114327907562, 0.013303371146321297, 0.008243135176599026, -0.02244790829718113, 0.014836988411843777, -0.022120941430330276, 0.009179891087114811, 0.0017816199688240886, 0.041691169142723083, 0.005526185967028141, 0.011160864494740963, 0.02374882623553276, 0.012609027326107025, -0.0022528746630996466, 0.005812540650367737, 0.004120551981031895, 0.006874788086861372, -0.0011651975801214576, -0.001832381822168827, -0.001228270586580038, 0.01802785135805607, -0.005278395488858223, -0.010725121945142746, 0.005258272867649794, 0.008830958046019077, 0.01409714762121439, -0.02889464609324932, -0.001450310810469091, -0.047569312155246735, -0.00920156855136156, 0.004153953865170479, -0.018607480451464653, 0.007250675465911627, -0.010692236013710499, -0.01584869623184204, 0.05646444484591484, -0.006540016271173954, -0.004483573138713837, -0.009804436936974525, -0.017676135525107384, -0.009097610600292683, 0.025004038587212563, 0.010966276749968529, 0.005269411485642195, 0.02735844813287258, -0.017337271943688393, 0.00571989081799984, -0.010263681411743164, 0.007580539211630821, 0.002401940291747451, -0.0067969998344779015, -0.001359616289846599, 0.008712044917047024, 0.02016257308423519, 0.015748189762234688, -0.01070936769247055, 0.010186650790274143, 0.01822623237967491, -0.012284214608371258, -0.00473607424646616, -0.008414201438426971, 0.00447540357708931, -0.01420158613473177, 0.020297441631555557, 0.0280614010989666, 0.018713394179940224, -0.0077304840087890625, -0.01462982501834631, 0.15605881810188293, -0.004211545456200838, 0.0038541227113455534, 0.01266256533563137, 0.014143969863653183, 0.013340877369046211, 0.021967941895127296, -0.006989834364503622, 0.017395542934536934, -0.005411892663687468, -0.020678211003541946, 0.012266227975487709, -0.0033954130485653877, 0.0012332285987213254, -0.016151325777173042, -0.014301598072052002, 0.003349518170580268, -0.027506548911333084, 0.023518187925219536, -0.006149211432784796, -0.02092776633799076, -0.01460941880941391, -0.0012642777292057872, -0.0038283702451735735, -0.029304519295692444, -0.00010851673141587526, -0.004149585496634245, 0.02552252821624279, -0.017868435010313988, 0.003146624192595482, 0.010213378816843033, 1.6215431969612837e-05, 0.030160684138536453, -0.01813052035868168, -0.01758938655257225, 0.004767769016325474, 0.012031057849526405, -0.0005329383420757949, -0.008707622066140175, 0.005478938575834036, -0.009966708719730377, 0.024113714694976807, 0.006523939315229654, 0.01196730975061655, -0.02757124789059162, 0.200795516371727, 0.007405104115605354, 0.008039125241339207, 0.018737666308879852, 0.009460900910198689, 0.0003555542789399624, -0.005531535483896732, 0.013351303525269032, 0.02378748543560505, 0.02233627624809742, 0.0016332898521795869, -7.110490696504712e-05, -0.008858252316713333, 0.0068496353924274445, -0.029436510056257248, 0.004679437261074781, 0.006671624258160591, 0.00021922800806351006, -0.010709901340305805, -0.020260054618120193, 3.37276651407592e-05, 0.0109724011272192, -0.011331710033118725, -0.0048538981936872005, -0.01475105807185173, 0.00852289516478777, 0.005434667691588402, 0.009040392003953457, 0.012687873095273972, 0.009946622885763645, -0.005790078081190586, -0.006860286928713322, -0.028284572064876556, 0.014708089642226696, -0.0002985840546898544, -0.006213660351932049, -0.008630531840026379, -0.004831978585571051, -0.000976721988990903, -0.01416360680013895, -0.0037039355374872684, -0.004098815843462944, 0.014447378925979137, 0.029721127822995186, -0.008668721653521061, -0.00333578628487885, -0.0029913061298429966, 0.01232840120792389, 0.008163326419889927, 0.003918318543583155, 0.02631228230893612, 0.008715867064893246, -0.011344535276293755, 0.02076593041419983, 0.005838864482939243, 0.010892540216445923, 0.002628169720992446, -0.0025816084817051888, -0.0331055223941803, 0.014574280008673668, -0.011587711982429028, 0.014369508251547813, 0.0028302972204983234, 0.012692592106759548, 0.005942538846284151, -0.005189818330109119, -0.004026086535304785]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Hugging Face\n",
        "model_name = \"all-MiniLM-L6-v2\" # @param [\"BAAI/bge-en-icl\",\"all-MiniLM-L6-v2\"]\n",
        "query = \"India is a growing country\" # @param {\"type\":\"string\",\"placeholder\":\"India is a growing country\"}\n",
        "huggingface_embeddings=HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "result = huggingface_embeddings.embed_query(query)\n",
        "print(len(result),result)"
      ],
      "metadata": {
        "id": "Tn8JwvZQ8SHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare for VB"
      ],
      "metadata": {
        "id": "GJ9ebLPD841g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary for the Text"
      ],
      "metadata": {
        "id": "Bp6LQQRk80QQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "GiFNjs7k82r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text=\"\"\"You are an assistant tasked with summarizing text for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw text elements. \\\n",
        "    Give a concise summary of the table or text that is well optimized for retrieval.text: {element} \"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt_template | open_ai_model | StrOutputParser()\n",
        "\n",
        "summarize_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCf1-yhQ9KB0",
        "outputId": "4f5438be-7ee0-4a1e-cb13-350b6fdc118e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+ \n",
            "| Parallel<element>Input | \n",
            "+------------------------+ \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "        +--------+         \n",
            "        | Lambda |         \n",
            "        +--------+         \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "  +--------------------+   \n",
            "  | ChatPromptTemplate |   \n",
            "  +--------------------+   \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +-----------------+    \n",
            "    | StrOutputParser |    \n",
            "    +-----------------+    \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_summary=summarize_chain.batch(NarrativeText,{\"max_concurrency\": 5})\n",
        "text_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxX8XSzG-6qW",
        "outputId": "c5a6ce1b-3515-4ece-b7d8-372e7e9ca65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Authors: Patrick Lewis and Ethan Perez',\n",
              " 'Authors: Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler.',\n",
              " 'This text lists individuals associated with a project or paper: Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. The presence of symbols † and ‡ suggests affiliations or contributions are indicated but not detailed in the text.',\n",
              " 'This text lists affiliations: Racebook AI Research, University College London, and New York University.',\n",
              " 'Email address: plewis@fb.com',\n",
              " 'This text examines retrieval-augmented generation (RAG) models that combine pre-trained parametric memory with non-parametric memory for language tasks. Unlike traditional language models, these RAG models incorporate a seq2seq framework linked to a dense vector index of Wikipedia, accessed via a pre-trained neural retriever. Two RAG approaches are compared: one using consistent retrieved passages for the entire sequence and another using varied passages per token. RAG models demonstrate superior performance on knowledge-intensive NLP tasks, setting new benchmarks on open-domain QA tasks and surpassing traditional seq2seq and task-specific models. Additionally, RAG models produce more specific, diverse, and factual outputs on language generation tasks.',\n",
              " 'Pre-trained neural language models demonstrate extensive in-depth learning from data without external memory but have limitations like difficulty in expanding memory, lack of transparency, and potential for generating inaccuracies. Hybrid models, merging parametric with non-parametric memory, address these issues by allowing direct revision and interpretation of knowledge. Models like REALM and ORQA, which integrate masked language models with differentiable retrievers, have shown promising advancements in this area.',\n",
              " 'The approach involves combining a pre-trained retriever (using a Query Encoder and Document Index) with a pre-trained seq2seq model (Generator) and fine-tuning them end-to-end. It uses Maximum Inner Product Search (MIPS) to identify the top-K relevant documents for a query. For final predictions, it treats the retrieved documents as latent variables and marginalizes over seq2seq predictions for each document.',\n",
              " 'This text discusses the integration of hybrid parametric and non-parametric memory into sequence-to-sequence (seq2seq) models, expanding beyond open-domain extractive question answering.',\n",
              " 'The Retrieval-Augmented Generation (RAG) approach enhances pre-trained generation models by integrating non-parametric memory through a general-purpose fine-tuning method. RAG models combine a seq2seq transformer as parametric memory with a dense vector index of Wikipedia as non-parametric memory, accessed by a pre-trained neural retriever (DPR). This combination forms a probabilistic, end-to-end trained model. The retriever provides latent documents based on input, which the seq2seq model (such as BART) conditions on, along with the input, to generate outputs. The model utilizes a top-K approximation for marginalizing latent documents, applicable either per-output or per-token basis. Similar to models like T5 or BART, RAG can be jointly fine-tuned on various seq2seq tasks.',\n",
              " 'The text discusses contrasting approaches in integrating memory systems into architectures. Traditional methods train non-parametric memory from scratch for specific tasks, such as memory networks and stack-augmented networks. This work differs by exploring pre-trained parametric and non-parametric memory components, emphasizing pre-trained access mechanisms for retrieving knowledge without the need for further training.',\n",
              " 'The study demonstrates the advantages of integrating parametric and non-parametric memory for knowledge-intensive tasks, achieving state-of-the-art results in open Natural Questions, WebQuestions, and CuratedTrec, and outperforming recent specialized pre-training methods on TriviaQA. Unconstrained generation surpasses extractive approaches even in extractive tasks. In experiments with MS-MARCO and Jeopardy question generation, the models produce more factual, specific, and diverse responses compared to a BART baseline. For FEVER fact verification, results are within 4.3% of the best retrieval-supervised models. Non-parametric memory is shown to be adaptable, allowing for model updates as knowledge evolves.',\n",
              " 'RAG models utilize an input sequence x to retrieve relevant text documents z, which serve as additional context for generating a target sequence y. These models consist of two key components: a retriever pη(z|x) with parameters η that provides top-K truncated distributions over text passages based on the query x, and a generator pθ(yi|x,z,y1:i−1) responsible for generating sequences.',\n",
              " 'Open-source code to run experiments with Retrieval-Augmented Generation (RAG) is available in the Hugging Face Transformers Library. The code is accessible at the GitHub repository: https://github.com/huggingface/transformers/blob/master/examples/rag/. Additionally, an interactive demo of RAG models can be explored at https://huggingface.co/rag/.',\n",
              " 'Generates a current token based on previous tokens, the original input, and a retrieved passage.',\n",
              " \"Summary: This text introduces two models, RAG-Sequence and RAG-Token, for end-to-end retrieval and generation using retrieved documents as latent variables. RAG-Sequence uses the same document for predicting all target tokens, while RAG-Token can use different documents for each token. It details the models' formal introduction, alongside their components (pη and pθ) and the training and decoding procedures.\",\n",
              " 'The RAG-Sequence model marginalizes a single retrieved document, treated as a latent variable, to generate a full sequence using seq2seq probability p(y|x) via a top-K approximation. It retrieves the top K documents, and the generator computes output sequence probabilities for each, which are then marginalized.',\n",
              " 'The RAG-Token model enables dynamic content selection by drawing a different latent document for each target token and marginalizing the results. It utilizes the top K documents retrieved to produce a token distribution for each document, iteratively generating an answer by marginalizing over these distributions for each successive token.',\n",
              " 'RAG can perform sequence classification by treating the target class as a single-target sequence, making RAG-Sequence and RAG-Token equivalent.',\n",
              " 'The retrieval component pη(z|x) utilizes DPR, which employs a bi-encoder architecture.',\n",
              " 'A document retrieval system uses a bi-encoder architecture initialized from DPR, based on BERTBASE, to create dense representations of documents and queries. The system solves a Maximum Inner Product Search (MIPS) problem to retrieve the top-k documents with the highest prior probability of containing answers to TriviaQA and Natural Questions. The document index is referred to as non-parametric memory and achieves efficient retrieval through these dense representations.',\n",
              " 'The generator component pθ(yi|x,z,y1:i−1) is implemented using BART-large, a 400M parameter pre-trained seq2seq transformer model, known for its state-of-the-art performance in generation tasks. The input x is concatenated with the retrieved content z for generation. BART was pre-trained with a denoising objective using various noising functions and outperforms similarly sized T5 models. The BART generator parameters are referred to as the parametric memory.',\n",
              " 'Joint training of retriever and generator components is conducted without direct supervision for document retrieval, using a fine-tuning corpus of input/output pairs.',\n",
              " 'Optimize retrieval by minimizing the negative marginal log-likelihood of each target.',\n",
              " 'The process utilizes stochastic gradient descent with Adam for optimization. Unlike REALM, the document encoder BERTd, and its index, are kept fixed to avoid costly updates, while only the query encoder BERTq and the BART generator are fine-tuned for improved performance.',\n",
              " 'RAG-Sequence and RAG-Token employ distinct methods to approximate argmaxy p(y|x) during test time.',\n",
              " 'RAG-Token is an autoregressive seq2seq generator model. It determines transition probability with a formula involving top-k probabilities and decodes using a standard beam decoder with Po(yi |x, y1i—1).',\n",
              " 'RAG-Sequence involves decoding complexities because the likelihood \\\\( p(y|x) \\\\) does not neatly decompose into a per-token likelihood. Instead, beam search is performed for each document \\\\( z \\\\), and hypotheses are evaluated using \\\\( p_\\\\theta(y_i|x,z,y_{1:i-1}) \\\\). \"Thorough Decoding\" includes additional forward passes for documents where the hypothesis does not appear, whereas \"Fast Decoding\" approximates that \\\\( p_\\\\theta(y|x,z_i) \\\\approx 0 \\\\) where \\\\( y \\\\) was not initially generated, streamlining the process by avoiding extra forward passes after generating the candidate set \\\\( Y \\\\).',\n",
              " 'The study explores using RAG (Retrieval-Augmented Generation) across various knowledge-intensive tasks, employing a December 2018 Wikipedia dump as the non-parametric knowledge base. The dump is split into 100-word chunks, creating 21 million documents, and a FAISS index is built for efficient retrieval. During training, the model retrieves the top k documents (k ∈ {5,10}) for each query, with k being adjusted for test time based on development data. Details of experiments for each task are provided.',\n",
              " \"The text discusses open-domain question answering (QA), focusing on the RAG model. RAG minimizes the negative log-likelihood of answers, operating with input-output text pairs. It's compared against extractive QA, which uses non-parametric knowledge from documents, and Closed-Book QA, relying on parametric knowledge without retrieval. Four datasets are used: Natural Questions (NQ), TriviaQA (TQA), WebQuestions (WQ), and CuratedTrec (CT). CT and WQ models are initialized with NQ RAG models due to their small size. The same data splits as past studies are used, with performance measured by Exact Match (EM) scores. TQA results are compared to T5 on the TQA Wiki test set.\",\n",
              " 'RAG models enhance beyond extractive QA with free-form, abstractive text generation, tested using the MSMARCO NLG task v2.1. This task includes questions, ten gold passages from a search engine for each question, and full sentence answers derived from these passages. In this evaluation, only the questions and answers are utilized, excluding the passages.',\n",
              " \"MSMARCO is an open-domain abstractive QA task where some questions require gold passages for accurate answers, like specific weather queries. Performance drops without these passages, and many questions aren't answerable using only Wikipedia. RAG can leverage parametric knowledge for reasonable responses.\",\n",
              " 'This text evaluates RAG\\'s capabilities in generating open-domain questions, specifically through the challenging task of creating Jeopardy-style questions. Unlike standard open-domain QA tasks with straightforward questions, Jeopardy requires precise and factual question generation based on specific answer entities, such as generating \"In 1986 Mexico scored as the first country to host this international sports competition twice\" for the answer \"The World Cup.\" This task tests the model\\'s ability in knowledge-intensive generation.',\n",
              " 'The study utilizes SearchQA splits (100K train, 14K dev, 27K test samples) to train a BART model for a new task, evaluated with the SQuAD-tuned Q-BLEU-1 metric, known for prioritizing entity matches and aligning closely with human judgment for question generation. Two human evaluations assess generation factuality (corroboration by trusted sources) and specificity (mutual dependence between input and output). A pairwise comparative evaluation is employed, where evaluators compare questions generated by BART and RAG, choosing which question is better, both are good, or neither is good.',\n",
              " \"FEVER is a task that involves classifying natural language claims as supported, refuted, or not enough information based on evidence retrieved from Wikipedia. It combines evidence retrieval with entailment reasoning and tests models' ability to classify rather than generate. The task is notable for not using supervised evidence retrieval, making it applicable to scenarios lacking retrieval supervision. FEVER supports both a 3-way classification (supports/refutes/not enough info) and a 2-way classification (supports/refutes). The system reports label accuracy for both variations.\",\n",
              " \"RAG sets a new state of the art in open-domain QA tasks by combining generation flexibility with high performance without requiring specialized pre-training. Unlike REALM and T5+SSM, it achieves strong results without salient span masking. RAG's retriever is initialized with DPR's retriever, leveraging retrieval supervision on Natural Questions and TriviaQA. It surpasses systems like DPR, which rely on BERT-based re-rankers and extractive readers, proving these components unnecessary for achieving top performance.\",\n",
              " 'Advantages of generating answers include leveraging documents with clues, even if they do not contain verbatim answers. This allows for creating correct responses when extractive methods fall short.',\n",
              " 'RAG improves document marginalization, generating correct answers even without them in retrieved documents, achieving 11.8% accuracy on NQ, outperforming extractive models scoring 0%.',\n",
              " 'RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu and Rouge-L points, nearing state-of-the-art performance despite not using gold passages. RAG models generate factually correct text with less hallucination and more diversity compared to BART.',\n",
              " 'RAG-Token outperforms RAG-Sequence and both surpass BART in Jeopardy question generation, as measured by Q-BLEU-1. In human evaluations of 452 generation pairs, RAG-Token is rated more factual than BART in 42.7% of instances versus 7.1% for BART. RAG and BART are both deemed factual in 17% of cases. RAG also produces more specific outputs, highlighting its superiority over BART for this task. Table 3 provides typical model outputs.',\n",
              " 'RAG-Token is effective in answering Jeopardy-like questions by combining information from multiple documents. When generating answers like \"The Sun\" and \"A Farewell to Arms\", document posteriors initially guide the generation. However, the model’s inherent knowledge often allows it to complete titles without specific document reliance, as seen when a BART model, without external documents, completes partial encodings like \"The Sun\" into \"The Sun Also Rises\". This interplay highlights how RAG-Token\\'s non-parametric component guides and extracts information stored in its parametric memory.',\n",
              " \"Table 2 presents FEVER results, showing RAG's 3-way classification performance within 4.3% of state-of-the-art models, despite RAG's simpler architecture and lack of intermediate retrieval supervision.\",\n",
              " 'The text discusses a prominent author whose works are classics of American literature and highlights his novels \"A Farewell to Arms\" (1929) based on wartime experiences and \"The Sun Also Rises\" (1926), marking his debut. It also mentions his association with the 1920s \"Lost Generation\" expatriate community.',\n",
              " 'The provided text appears to be a series of non-descriptive characters or symbols and does not contain coherent information to summarize.',\n",
              " 'The text describes Figure 2, illustrating RAG-Token document posterior probabilities for each generated token from the input \"Hemingway\" in Jeopardy generation using 5 retrieved documents. Document 1 shows a high posterior for \"A Farewell to Arms,\" while document 2 is high for \"The Sun Also Rises.\"',\n",
              " \"Table 3 provides examples from generation tasks, illustrating that RAG models produce more specific and factually accurate responses compared to alternatives. It uses '?' to denote factually incorrect answers, and '*' for partially correct ones.\",\n",
              " \"The study compares RAG's performance in a 2-way classification task with Thorne and Vlachos's RoBERTa model, which uses gold evidence sentences to classify claims as true or false. RAG, relying only on claims and self-retrieved evidence, achieves an accuracy within 2.7% of the RoBERTa model. Analysis shows that 71% of RAG's top-retrieved documents match gold evidence articles, and in 90% of cases, a gold article is among the top 10 retrieved documents.\",\n",
              " 'RAG models outperform BART in factuality and specificity for Jeopardy question generation. RAG-Sequence produces more diverse ngrams than RAG-Token, and both RAG models are significantly more diverse than BART even without diversity-promoting decoding.',\n",
              " 'The study examines the effectiveness of learned retrieval in RAG by conducting ablations with a frozen retriever during training, demonstrating that active learned retrieval enhances task results across the board, as indicated in Table 6.',\n",
              " \"The comparison between RAG's dense retriever and a word overlap-based BM25 retriever reveals that BM25 excels in the FEVER task due to its entity-centric nature, suiting word overlap methods. However, differentiable retrieval enhances performance across other tasks, notably in Open-Domain QA, where it is essential.\",\n",
              " \"Non-parametric memory models such as RAG allow knowledge updates at test time, unlike parametric-only models like T5 or BART, which require additional training. An example is given using indices from DrQA's December 2016 Wikipedia dump compared to December 2018 data, highlighting how RAG's outputs reflect updates in world leader changes.\",\n",
              " 'Summary of Table 4: Evaluation results of human assessments on the performance of Jeopardy Question Generation Task.',\n",
              " \"The text discusses using a template “Who is {position}?” to query a RAG model about world leaders. The model correctly identifies leaders 70% of the time with a 2016 index for 2016 leaders, and 68% with a 2018 index for 2018 leaders. Accuracy drops significantly when using mismatched indices, highlighting that RAG's world knowledge can be updated by changing its non-parametric memory.\",\n",
              " \"The study examines how retrieving varying numbers of latent documents impacts model performance in Open-domain QA. Training with 5 or 10 documents shows no significant performance differences, but adjusting retrieval numbers at test time affects results. Increasing document retrieval consistently enhances RAG-Sequence performance, but for RAG-Token, performance peaks at 10 documents. More documents improve RAG-Token's Rouge-L scores but slightly decrease Bleu-1, with minimal impact on RAG-Sequence.\",\n",
              " 'The text discusses the benefits of using retrieval to enhance various NLP tasks in isolation, such as open-domain question answering, fact-checking, fact completion, long-form question answering, Wikipedia article generation, dialogue, translation, and language modeling. It highlights the unification of these successful strategies into a single retrieval-based architecture, demonstrating its capability to perform well across multiple tasks.',\n",
              " 'This text discusses the development of general-purpose architectures for NLP tasks, highlighting the effectiveness of single, pre-trained language models like GPT-2, BART, and T5. These models excel in both discriminative and generative tasks without needing retrieval. The text mentions efforts to further enhance performance by integrating a retrieval module within unified generative models, broadening the range of possible tasks.',\n",
              " 'The text discusses advances in learned retrieval within information retrieval systems, particularly focusing on the use of pre-trained neural language models. It highlights existing research that enhances retrieval modules for specific tasks, like question answering, using methods such as search, reinforcement learning, and latent variable approaches. Unlike these specialized approaches, the text introduces a single retrieval-based architecture that can be fine-tuned to perform well across multiple tasks.',\n",
              " 'The text discusses memory-based architectures in neural networks, focusing on a document index that serves as a large, external memory of raw text, enhancing interpretability and enabling dynamic updates. Unlike other approaches that use trained embeddings, this model retrieves raw text, making it human-readable and writable. This method has applications in knowledge-intensive dialogue systems, where retrieval is typically achieved via TF-IDF rather than learned end-to-end retrieval. Recent work aims to improve dialogue models with fact embeddings and learn to retrieve embeddings for entities in the input.',\n",
              " 'This text describes a retrieve-and-edit methodology where similar input-output pairs are retrieved and edited for a final output, commonly used in domains like Machine Translation and Semantic Parsing. Unlike traditional approaches, this method emphasizes aggregating content from multiple retrieved items and focuses on learning latent retrieval and retrieving evidence documents. RAG (retrieval-augmented generation) techniques are suggested as potential future improvements.',\n",
              " \"This study introduces hybrid generation models using both parametric and non-parametric memory, achieving state-of-the-art results in open-domain QA with RAG models. Users preferred RAG over BART for its factual and specific output. The effectiveness of RAG's learned retrieval component was validated, and its retrieval index can be updated without retraining. Future research may explore joint pre-training of components. This work highlights new research potential in combining parametric and non-parametric memories for various NLP tasks.\",\n",
              " 'This work improves upon previous efforts by being strongly grounded in factual knowledge, primarily from Wikipedia, which reduces inaccurate information and increases control and interpretability. It can benefit society by being applied in diverse scenarios, such as using a medical index for open-domain questions or enhancing job performance.',\n",
              " 'This text discusses potential downsides of using advanced language models like RAG, such as generating biased, misleading, or fake content, impersonation, and automating spam/phishing. It highlights concerns about job automation and suggests employing AI systems to combat these risks.',\n",
              " 'The authors express gratitude to reviewers for their feedback, HuggingFace for assistance with open-sourcing RAG models, and Kyunghyun Cho and Sewon Min for their discussions and advice. EP receives support from the NSF Graduate Research Fellowship, and PL is supported by the FAIR PhD program.',\n",
              " 'A conference paper presented at the Association for Computational Linguistics in Florence, Italy, in July 2019, pages 6086–6096. It is published in the Computational Linguistics anthology and can be accessed via URL: https://www.aclweb.org/anthology/P19-1612. DOI: 10.18653/v1/P19-1612.',\n",
              " 'This document refers to a workshop paper from CoCoNIPS 2016, co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016) in Barcelona, Spain, on December 9, 2016. It is published in volume 1773 of the CEUR Workshop Proceedings. Additional information and the full paper can be accessed via the URL: http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf.',\n",
              " \"The document discusses testing setups for Open-domain QA using RAG models. RAG-Token models test with 15 retrieved documents, while RAG-Sequence models use 50, with Thorough Decoding due to short answers, and greedy decoding for QA. For Open-MSMarco and Jeopardy, 10 documents are retrieved for both model types, with a BART-large model as a baseline, using a beam size of four and Fast Decoding for RAG-Sequence as Thorough Decoding didn't enhance results.\",\n",
              " 'Figure 4 displays the annotation interface used for human evaluation of factuality, with an option to view detailed instructions and a worked example via a \"view tool guide\" pop-out.',\n",
              " 'The text describes the user interface for human evaluation, highlighting measures to prevent bias by randomizing model associations with sentences. Annotators received detailed instructions and used the internet for research. To ensure accuracy, gold sentences were included, and annotations from two underperforming annotators were removed.',\n",
              " \"This text discusses the training and implementation details of RAG models and BART baselines using Fairseq, leveraging mixed precision arithmetic and distributed training on 8 NVIDIA V100 GPUs. Maximum Inner Product Search with FAISS is used for document indexing, requiring ~100GB of CPU memory, which is reduced to 36GB through compression. The code has been ported to HuggingFace Transformers to enhance usability and open-source access, with equivalent performance to the previous version. Resources, including scripts and an interactive demo, are available on HuggingFace's GitHub and website links.\",\n",
              " 'Fairseq and Hugging Face Transformers are two prominent libraries available on GitHub for building and deploying machine learning models. Fairseq focuses on sequence modeling, especially in natural language processing (NLP), while Hugging Face Transformers provide tools and pre-trained models for various NLP tasks.',\n",
              " 'In open-domain QA, utilizing multiple answer annotations improves model training by matching answers within documents. This strategy is used by extractive models and the RAG model, which enhances accuracy by training on individual (question, answer) pairs. For TriviaQA, unsuitable answer candidates, like emojis or spelling variants, are filtered out if they do not appear in the top 1000 documents for a query.',\n",
              " 'CuratedTrec preprocessing involves handling answers formatted as regex, deemed unsuitable for answer-generation models. To address this, the process retrieves the top 1000 documents per query, using the most frequent regex match as the target. If no matches appear, a heuristic generates all regex permutations, replacing non-deterministic symbols with whitespace.',\n",
              " 'The text discusses evaluation setups for the TriviaQA dataset in open-domain QA. It contrasts the use of the public TriviaQA Web Development split, following common practice, with the official Wikipedia test set as used by Roberts et al. [52]. Results are reported on both setups to ensure fair comparison. The study found better performance with the Wikipedia test set, suggesting its questions are simpler to answer from Wikipedia.',\n",
              " 'The FEVER classification process involves regenerating a claim, classifying it using the final hidden state\\'s representation, and marginalizing across documents to determine class probabilities. This involves deciding if a claim is \"Supported,\" \"Refuted,\" or \"Not Enough Info.\" An additional sub-task involves extracting sentences from Wikipedia for evidence, a challenge due to differing Wikipedia dumps, which is a matter planned for future work.',\n",
              " 'The experiment added a \"Null document\" mechanism to RAG to handle cases where no useful information could be retrieved, similar to REALM [20]. This involved \"retrieving\" an empty document and predicting a logit for it, then marginalizing over k + 1 predictions. Methods tested included learning a null document embedding, a static bias term, or using a neural network to predict the logit. None of these methods improved performance, so they were omitted for simplicity. For Open MS-MARCO, the model tends to retrieve a consistent set of documents for less retrieval-beneficial questions, indicating that a null document mechanism may not be necessary for RAG.',\n",
              " 'The RAG models utilize a BERT-base query encoder and a document encoder from DPR, each with 110M parameters (document encoder remains untrained), alongside BART-large with 406M parameters, totaling 626M trainable parameters.',\n",
              " 'T5-11B, a \"closed-book\" model with 11 billion parameters, is the top-performing open-domain QA model. T5-large, with 770 million parameters, scores 28.9 EM on Natural Questions, significantly lower than the hybrid RAG-Sequence model\\'s 44.5, illustrating that hybrid models with fewer trainable parameters outperform in QA tasks. The non-parametric memory index contains 21 million 728-dimensional vectors, equaling 15.3 billion values, and can be stored efficiently using 8-bit floating point precision.',\n",
              " 'In preliminary experiments on tasks like story generation, the retrieval component sometimes \"collapsed,\" retrieving the same documents irrespective of input, leading the generator to ignore these documents. As a result, the RAG model performed similarly to BART. This collapse may stem from a reduced need for factual knowledge or longer target sequences impacting the retriever\\'s gradient informativeness. Perez et al. also noted inaccurate retrieval outcomes when tuning the retrieval component for better downstream task performance.',\n",
              " 'Summary: Table 7 displays the quantity of training, development, and test datapoints for each dataset.']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_summary[3]"
      ],
      "metadata": {
        "id": "zXE0m8li_xxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text:str):\n",
        "  return re.sub(r\"<think>.*?</think>\\s*\",\"\",text,flags=re.DOTALL)\n"
      ],
      "metadata": {
        "id": "ELw8uYOMAxdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary for Table"
      ],
      "metadata": {
        "id": "RojPc8dSByV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_text = \"\"\"You are an AI Assistant tasked with summarizing tables for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw table elements. \\\n",
        "    Give a concise summary of the table that is well optimized for retrieval. Table:{element} \"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | open_ai_model | StrOutputParser()\n",
        "\n",
        "\n",
        "summarize_chain.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0h-UNYrBWex",
        "outputId": "30543d1a-cc4d-4a40-effb-4aa847174a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+ \n",
            "| Parallel<element>Input | \n",
            "+------------------------+ \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "        +--------+         \n",
            "        | Lambda |         \n",
            "        +--------+         \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "  +--------------------+   \n",
            "  | ChatPromptTemplate |   \n",
            "  +--------------------+   \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "      +------------+       \n",
            "      | ChatOpenAI |       \n",
            "      +------------+       \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "    +-----------------+    \n",
            "    | StrOutputParser |    \n",
            "    +-----------------+    \n",
            "             *             \n",
            "             *             \n",
            "             *             \n",
            "+-----------------------+  \n",
            "| StrOutputParserOutput |  \n",
            "+-----------------------+  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries = summarize_chain.batch(Table, {\"max_concurrency\": 5})\n",
        "table_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he8qBffeCEnW",
        "outputId": "0729cf55-4452-4cfb-8f30-55c6597dfde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This table presents a comparison of various models on their performance metrics, including B-1 (BLEU-1), QB-1, R-L (ROUGE-L), and Label Accuracy. Specific models featured are T5-11B, T5-11B+SSM, Open Realm, DPR, BART, RAG-Token, and RAG-Sequence, each showing diverse results. The table highlights Closed Book settings, with metrics like 52, 34.5, and accuracy figures like 40.4 and 44.1 for specific models, alongside Open REALM and Book performance measures such as 49.8*, 49.9*, and ranges up to 89.5. Additionally, there are references to state-of-the-art (SotA) benchmarks. The data is displayed in a structured format to enable comparison of model capabilities under various task configurations.',\n",
              " 'The table contrasts multiple AI model generations for various tasks and inputs. For defining the \"middle ear,\" BART provides an incorrect looped definition, RAG-T describes it accurately as being internal to the eardrum, and RAG-S specifies it includes the tympanic cavity and ossicles. Regarding the currency needed in Scotland, BART and RAG-S correctly identify it as pound sterling, while RAG-T simplifies it to pound. For the largest number of counties in a U.S. state, BART misinterprets with an off-topic answer, and RAG model generations provide unrelated facts about the state of Washington. Lastly, for summarizing \"The Divine Comedy,\" BART and RAG-S focus on the poem\\'s structure into three parts, while RAG-T highlights \"Inferno\" as its first section.',\n",
              " 'The table compares factuality and specificity metrics across different models and datasets. It includes performance percentages for BART, RAG-Token, and RAG-Seq in various conditions, alongside categories like \"BART better,\" \"RAG better,\" \"Both good,\" \"Both poor,\" and \"No majority.\" For factuality and specificity, BART, RAG-Token, and RAG-Seq are evaluated across MSMARCO, Jeopardy, and QGen datasets, with specific percentages indicating model superiority or parity. The Gold standard holds the highest factuality percentage (89.6%), followed closely by RAG-Seq (83.5%), whereas MSMARCO shows a higher specificity for BART (90.0%) compared to RAG-Token (70.7%) and RAG-Seq (77.8%).',\n",
              " 'This table compares the performance of various RAG models (RAG-Token-BM25, RAG-Sequence-BM25, RAG-Token-Frozen, RAG-Sequence-Frozen, RAG-Token, and RAG-Sequence) across different datasets and metrics. The datasets include Natural Questions (NQ), TriviaQA (TQA), WebQuestions (WQ), CuratedTREC (CT), Jeopardy-QGen, MSMarco, and FVR-3. The performance metrics used are Exact Match, B-1 (BLEU-1), QB-1, R-L (ROUGE-L), and B-1 for certain tasks. RAG-Token and RAG-Sequence generally outperform their Frozen and BM25 counterparts in most metrics, particularly with higher scores in Exact Match and ROUGE-L, indicating their superior retrieval and generation capabilities.',\n",
              " 'This table provides a detailed breakdown of different datasets used for various natural language processing tasks across distinct sets: Train, Development, and Test. The tasks and corresponding datasets are as follows: Natural Questions, TriviaQA, WebQuestions, CuratedTrec, Jeopardy Question Generation, MS-MARCO, FEVER-3-way, and FEVER-2-way. Each dataset is listed with the number of examples or instances available for each set. Notably, MS-MARCO has asterisked test data indicating a special consideration or note. The Train set generally contains the largest number of examples, followed by Development and then Test. The table reflects the diversity and distribution of data points across tasks, useful for retrieving dataset specifics in query-answering and question generation tasks.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary for Images"
      ],
      "metadata": {
        "id": "9kybQMvJCMYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import base64, os\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "\n",
        "def encode_image(image_path):\n",
        "  with(open(image_path,\"rb\")) as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def image_summarize(imag_base64,prompt):\n",
        "\n",
        "  msg = gemini_model.invoke(\n",
        "      [\n",
        "          HumanMessage(\n",
        "              content=[\n",
        "                  {\n",
        "                      \"type\":\"text\",\n",
        "                      \"text\":prompt\n",
        "                  },\n",
        "                  {\n",
        "                      \"type\":\"image_url\",\n",
        "                      \"image_url\":{\n",
        "                          \"url\": f\"data:image/jpeg;base64,{imag_base64}\"\n",
        "                      }\n",
        "                  }\n",
        "              ]\n",
        "          )\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  return msg.content\n",
        "\n",
        "def generate_imag_summaries(path):\n",
        "\n",
        "  image_base64_list = []\n",
        "\n",
        "  image_summarises = []\n",
        "\n",
        "  prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw image. \\\n",
        "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
        "\n",
        "\n",
        "  for img_file in sorted(os.listdir(path)):\n",
        "    if img_file.endswith(\".jpg\") or img_file.endswith(\".jpeg\") or img_file.endswith(\".png\"):\n",
        "      image_path = os.path.join(path, img_file)\n",
        "      image_base64 = encode_image(image_path)\n",
        "      image_base64_list.append(image_base64)\n",
        "      image_summaries = image_summarize(image_base64,prompt)\n",
        "      image_summarises.append(image_summaries)\n",
        "\n",
        "  return image_base64_list, image_summarises"
      ],
      "metadata": {
        "id": "IrE0kq96CIkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_dir_path = \"extracted_data\"\n",
        "image_base64_list, image_summarises = generate_imag_summaries(image_dir_path)\n",
        "\n",
        "image_summarises"
      ],
      "metadata": {
        "id": "hQdxFSo-E9uR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8664628a-08d9-4e11-e478-82e809ab2107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hemingway authorship quiz:  \"The Sun Also Rises\" and birthplace fact check.  Multiple choice question.',\n",
              " 'Diagram of end-to-end backpropagation through a query and a generator showing a query encoder, retriever, document index, and generator with examples of question answering, fact verification, and question generation.',\n",
              " 'light-blue background with two dark-blue vertical bars',\n",
              " 'Three graphs showing NQ Exact Match, NQ Answer Recall@K, and Bleu-1/Rouge-L score for RAG-Tok and RAG-Seq models against Fixed DPR and BM25 baselines, plotted against the number of retrieved documents (K).',\n",
              " 'Dataset sizes for Natural Questions, TriviaQA, WebQuestions, CuratedTrec, Jeopardy Question Generation, MS-MARCO, FEVER-3-way, and FEVER-2-way question answering tasks.  Train, development, and test set sizes are provided.',\n",
              " 'Table comparing question answering model performance across multiple datasets (NQ, TQA, WQ, Jeopardy, MSMARCO, FVR3, FVR2).  Results include accuracy scores for different models (T5, REALM, DPR, RAG) and settings (closed-book, open-book).  Highlights key performance metrics for each model and dataset.',\n",
              " \"Table comparing model outputs (BART, RAG-T, RAG-S) for three different tasks: defining the middle ear, identifying Scotland's currency, and answering a Jeopardy question about Washington.  Includes model input for each task.\",\n",
              " \"Comparative analysis of BART and RAG models on factuality and specificity, showing RAG's superior performance (42.7% vs 7.1%) and MSMARCO/Jeopardy QGen results.  RAG-Seq achieves highest accuracy (83.5%/53.8%).\",\n",
              " 'Table showing model performance (RAG-Token-BM25, RAG-Sequence-BM25, RAG-Token-Frozen, RAG-Sequence-Frozen, RAG-Token, RAG-Sequence) across various question answering datasets (NQ, TQA, WQ, CT, Jeopardy-QGen, MSMarco, FVR-3, FVR-2) using exact match and label accuracy metrics.  High FVR-2 label accuracy.']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store data in vectordb"
      ],
      "metadata": {
        "id": "K-mitlr-FP7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "import uuid"
      ],
      "metadata": {
        "id": "mehUUbiuFMQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = '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'\n"
      ],
      "metadata": {
        "id": "J6IV5kS5h-sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multi_vector_retriver(\n",
        "    vectorstore,\n",
        "    text_summaries ,texts,\n",
        "    table_summaries,tables,\n",
        "    image_summarises, images\n",
        "):\n",
        "\n",
        "  retriever=MultiVectorRetriever(\n",
        "      vectorstore=vectorstore,\n",
        "      docstore=InMemoryStore(),\n",
        "      id_key=\"doc_id\",\n",
        "  )\n",
        "\n",
        "  def add_docs(retriever,doc_summaries,doc_contents):\n",
        "\n",
        "    doc_ids = [ str(uuid.uuid4()) for _ in range(len(doc_summaries))]\n",
        "\n",
        "    summary_docs = [\n",
        "        Document(\n",
        "            page_content=doc_summary,\n",
        "            metadata={\"doc_id\":doc_id}\n",
        "        )\n",
        "        for doc_summary,doc_id in zip(doc_summaries,doc_ids)\n",
        "    ]\n",
        "\n",
        "    retriever.vectorstore.add_documents(summary_docs)\n",
        "    retriever.docstore.mset(\n",
        "        list(zip(doc_ids,doc_contents))\n",
        "    )\n",
        "\n",
        "  add_docs(retriever,text_summaries,NarrativeText)\n",
        "  add_docs(retriever,table_summaries,Table)\n",
        "  add_docs(retriever,image_summarises,image_summarises)\n",
        "\n",
        "  return retriever\n",
        "\n",
        "# embedding_model=load_model(\"embedding\")\n",
        "\n",
        "vectorstore=Chroma(collection_name=\"MMRAG\",embedding_function=huggingface_embeddings)\n",
        "retriever_multi_vector=create_multi_vector_retriver(\n",
        "    vectorstore,\n",
        "    text_summary,NarrativeText,\n",
        "    table_summaries,Table,\n",
        "    image_summarises, image_base64_list\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mHlmNLhGGz4",
        "outputId": "668f8218-075b-4fda-c570-42d0e3123f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-4db8d4c6874a>:39: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectorstore=Chroma(collection_name=\"MMRAG\",embedding_function=huggingface_embeddings)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_multi_vector"
      ],
      "metadata": {
        "id": "aPo3YRgHJ5NV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb3aa28-9648-4e89-c590-a7be7da6df42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x79126e41d4d0>, docstore=<langchain_core.stores.InMemoryStore object at 0x79126e14f8d0>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_multi_vector.get_relevant_documents(\n",
        "    \"Why We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7230Xu4RVp85",
        "outputId": "ff849193-b99b-4a9d-d119-598aa47eaf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-1b27466b39d0>:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retriever_multi_vector.get_relevant_documents(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.',\n",
              " 'Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.',\n",
              " 'Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.',\n",
              " 'We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_multi_vector.invoke(\n",
        "    \"Why We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and fine-tune end-to-end?\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24tEjynDVxni",
        "outputId": "f1f3a901-f96b-4ca2-8342-9e7517facb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.',\n",
              " 'Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents.',\n",
              " 'Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.',\n",
              " 'We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs (xj,yj), we']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image data processing"
      ],
      "metadata": {
        "id": "ZlNfMRPXWdQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io, re\n",
        "from IPython.display import HTML, display\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "bJgmvES5V6rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_image_data(b64data)->bool:\n",
        "  image_signatures = {\n",
        "      b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
        "      b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
        "      b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
        "      b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    header = base64.b64decode(b64data)[:8] # Decode and get first 8 bytes\n",
        "    for sig, format in image_signatures.items():\n",
        "      if header.startswith(sig):\n",
        "        return True\n",
        "    return False\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "def looks_like_base64(sb):\n",
        "  return re.match(\"[A-za-z0-9+/]+[=]{0,2}$\",sb) is not None\n",
        "\n",
        "def resize_base64_image(base64_string,size=(128,128)):\n",
        "  img_data = base64.b64decode(base64_string)\n",
        "  img = Image.open(io.BytesIO(img_data))\n",
        "  resized_img = img.resize(size,Image.LANCZOS)\n",
        "\n",
        "  buffered = io.BytesIO()\n",
        "  resized_img.save(buffered, format=img.format)\n",
        "  return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "\n",
        "def split_image_text_types(docs):\n",
        "  \"\"\"\n",
        "  Split base64-encoded images and text\n",
        "  \"\"\"\n",
        "\n",
        "  b64_images = []\n",
        "  texts = []\n",
        "\n",
        "  for doc in docs:\n",
        "\n",
        "    if isinstance(doc, Document):\n",
        "      doc = doc.page_content\n",
        "    if looks_like_base64(doc) and is_image_data(doc):\n",
        "      doc =resize_base64_image(doc,size=(1300,600))\n",
        "      b64_images.append(doc)\n",
        "    else:\n",
        "      texts.append(doc)\n",
        "\n",
        "  return {\n",
        "      \"images\":b64_images,\n",
        "      \"texts\":texts\n",
        "  }\n",
        "\n",
        "def img_prompt_func(data_dict):\n",
        "  \"\"\"\n",
        "  Join the context into a single string\n",
        "  \"\"\"\n",
        "\n",
        "  print(data_dict)\n",
        "  formatted_text = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "\n",
        "  messages = []\n",
        "\n",
        "  if data_dict[\"context\"][\"images\"]:\n",
        "    for img in data_dict[\"context\"][\"images\"]:\n",
        "      messages.append(\n",
        "          {\n",
        "              \"type\":\"image_url\",\n",
        "              \"image_url\":{\n",
        "                  \"url\": f\"data:image/jpeg;base64,{img}\"\n",
        "              }\n",
        "          }\n",
        "      )\n",
        "  text_message = {\n",
        "      \"type\":\"text\",\n",
        "      \"text\": (\n",
        "          \"You are a helpful assistant.\\n\"\n",
        "          \"You will be given a mixed info(s) .\\n\"\n",
        "          \"Use this information to provide relevant information to the user question. \\n\"\n",
        "          f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
        "          \"Text and / or tables:\\n\"\n",
        "          f\"{formatted_text}\"\n",
        "      ),\n",
        "  }\n",
        "\n",
        "  messages.append(text_message)\n",
        "\n",
        "  return [HumanMessage(content=messages)]\n",
        "\n"
      ],
      "metadata": {
        "id": "aH58CGciWl8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "def muti_model_rag_chain(retriever):\n",
        "  chain = (\n",
        "      {\n",
        "          \"context\":retriever | RunnableLambda(split_image_text_types),\n",
        "          \"question\":RunnablePassthrough(),\n",
        "\n",
        "      }\n",
        "      | RunnableLambda(img_prompt_func)\n",
        "      | open_ai_model\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  return chain\n",
        "\n",
        "chain_mm_rag = muti_model_rag_chain(retriever_multi_vector)\n",
        "\n",
        "chain_mm_rag.get_graph().print_ascii()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CqushP8YeMu",
        "outputId": "610baafa-d0e3-4ed3-ec62-b348809b3881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              +---------------------------------+           \n",
            "              | Parallel<context,question>Input |           \n",
            "              +---------------------------------+           \n",
            "                     ****               ****                \n",
            "                  ***                       ***             \n",
            "                **                             ***          \n",
            " +----------------------+                         **        \n",
            " | MultiVectorRetriever |                          *        \n",
            " +----------------------+                          *        \n",
            "             *                                     *        \n",
            "             *                                     *        \n",
            "             *                                     *        \n",
            "+------------------------+                  +-------------+ \n",
            "| split_image_text_types |                  | Passthrough | \n",
            "+------------------------+                  +-------------+ \n",
            "                     ****               ****                \n",
            "                         ***         ***                    \n",
            "                            **     **                       \n",
            "              +----------------------------------+          \n",
            "              | Parallel<context,question>Output |          \n",
            "              +----------------------------------+          \n",
            "                                *                           \n",
            "                                *                           \n",
            "                                *                           \n",
            "                      +-----------------+                   \n",
            "                      | img_prompt_func |                   \n",
            "                      +-----------------+                   \n",
            "                                *                           \n",
            "                                *                           \n",
            "                                *                           \n",
            "                         +------------+                     \n",
            "                         | ChatOpenAI |                     \n",
            "                         +------------+                     \n",
            "                                *                           \n",
            "                                *                           \n",
            "                                *                           \n",
            "                      +-----------------+                   \n",
            "                      | StrOutputParser |                   \n",
            "                      +-----------------+                   \n",
            "                                *                           \n",
            "                                *                           \n",
            "                                *                           \n",
            "                    +-----------------------+               \n",
            "                    | StrOutputParserOutput |               \n",
            "                    +-----------------------+               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_mm_rag.invoke(\n",
        "    \"\"\"Explain any images / figures in the paper with Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance\\\n",
        "in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\n",
        "  \"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "QGA0wVHsds5B",
        "outputId": "f3464266-aee1-40be-f898-0da2013be096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context': {'images': [], 'texts': ['Three graphs showing NQ Exact Match, NQ Answer Recall@K, and Bleu-1/Rouge-L score for RAG-Tok and RAG-Seq models against Fixed DPR and BM25 baselines, plotted against the number of retrieved documents (K).', 'to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%.', 'Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signiﬁcant differences in performance between them. We have the ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.', 'In preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would “collapse” and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks.']}, 'question': 'Explain any images / figures in the paper with Left: NQ performance as more documents are retrieved. Center: Retrieval recall performancein NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\\n  '}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In the provided text, the references to \"Figure 3\" likely correspond to three separate graphs or charts each demonstrating different performance metrics as a function of the number of documents retrieved by various models.\\n\\n1. **Left: NQ (Natural Questions) Performance as More Documents are Retrieved**: \\n   - This graph shows the Exact Match score for the NQ dataset as more documents are retrieved. The model comparison includes RAG-Sequence and RAG-Token against the Fixed DPR and BM25 baselines.\\n   - The explanation highlights that as more documents are retrieved, there is a monotonic improvement in Open-domain QA results for RAG-Sequence, meaning its performance consistently improves.\\n   - For RAG-Token, the performance peaks when 10 documents are retrieved, suggesting an optimal point beyond which additional documents do not significantly contribute to performance improvements.\\n\\n2. **Center: Retrieval Recall Performance in NQ**:\\n   - In this context, \"Retrieval Recall@K\" likely refers to the recall metric, which measures how many of the relevant documents were retrieved when a certain number (K) of documents are retrieved.\\n   - This graph would demonstrate how well different models manage to retrieve relevant documents from the NQ dataset as K increases.\\n\\n3. **Right: MS-MARCO Bleu-1 and Rouge-L as More Documents are Retrieved**:\\n   - This graph assesses both Bleu-1 and Rouge-L scores, common metrics for evaluating text generation, applied to the MS-MARCO dataset.\\n   - As per the text, retrieving more documents results in higher Rouge-L scores for RAG-Token, indicating better performance in some qualitative aspect of text matching.\\n   - However, the trade-off is a less pronounced increase in Bleu-1 scores, suggesting that RAG-Token might prioritize achieving better recall or capturing more relevant information at the potential cost of precision in language generation.\\n   - For the RAG-Sequence model, this effect of document retrieval on Rouge-L and Bleu-1 scores is less pronounced, implying a more stable performance across varying numbers of retrieved documents.\\n\\nOverall, these graphs and their analysis provide insights into how retrieval affects the performance of different models on various datasets, highlighting differences in how the RAG-Sequence and RAG-Token models handle more extensive document retrieval differently.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0ysy2Wael2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}